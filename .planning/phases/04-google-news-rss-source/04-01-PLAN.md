---
phase: 04-google-news-rss-source
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/sources/_protocol.py
  - src/sources/google_news.py
  - src/sources/__init__.py
autonomous: true

must_haves:
  truths:
    - "A NewsSource Protocol defines the common search() interface for all news source adapters"
    - "GoogleNewsSource.search() fetches Google News RSS, parses entries, and returns list[ArticleRef]"
    - "Actual Google News RSS results for a heat term + India query are parsed into valid ArticleRef objects with title, url, source, date, language, state, and search_term"
    - "HTTP errors, timeouts, and empty results are handled gracefully -- search() returns empty list, never raises"
    - "GoogleNewsSource satisfies the NewsSource Protocol (isinstance check passes)"
  artifacts:
    - path: "src/sources/_protocol.py"
      provides: "NewsSource Protocol with async search() method signature"
      contains: "class NewsSource"
    - path: "src/sources/google_news.py"
      provides: "GoogleNewsSource class implementing Google News RSS search"
      contains: "class GoogleNewsSource"
    - path: "src/sources/__init__.py"
      provides: "Re-exports of NewsSource and GoogleNewsSource"
      contains: "NewsSource"
  key_links:
    - from: "src/sources/google_news.py"
      to: "src/sources/_protocol.py"
      via: "GoogleNewsSource satisfies NewsSource Protocol structurally"
      pattern: "isinstance.*NewsSource"
    - from: "src/sources/google_news.py"
      to: "src/models/article.py"
      via: "Constructs ArticleRef from parsed RSS entries"
      pattern: "ArticleRef\\("
    - from: "src/sources/google_news.py"
      to: "httpx + feedparser"
      via: "httpx.AsyncClient.get() fetches RSS XML, feedparser.parse() parses entries"
      pattern: "feedparser\\.parse\\(response\\.text\\)"
---

<objective>
Create the NewsSource Protocol (common interface for all news source adapters) and the GoogleNewsSource class that fetches and parses Google News RSS search results into ArticleRef objects.

Purpose: This is the primary news collection adapter. Phase 5 sources (NewsData.io, GNews) will implement the same Protocol. Phase 6 (query engine) will call search() across all sources.

Output: Three files -- Protocol definition, Google News adapter, and updated package re-exports.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-google-news-rss-source/04-RESEARCH.md
@src/models/article.py
@src/sources/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create NewsSource Protocol and GoogleNewsSource implementation</name>
  <files>
    src/sources/_protocol.py
    src/sources/google_news.py
  </files>
  <action>
**File 1: `src/sources/_protocol.py`**

Create the NewsSource Protocol using `typing.Protocol` (NOT abc.ABC -- per research decision). The Protocol defines one async method:

```python
@runtime_checkable
class NewsSource(Protocol):
    async def search(
        self,
        query: str,
        language: str,
        country: str = "IN",
        *,
        state: str = "",
        search_term: str = "",
    ) -> list[ArticleRef]: ...
```

Key details:
- Use `from __future__ import annotations` for forward references
- Import `ArticleRef` from `src.models.article`
- `@runtime_checkable` decorator so `isinstance(obj, NewsSource)` works at runtime
- `state` and `search_term` are keyword-only parameters -- they are caller context needed to construct ArticleRef objects, not search parameters. All sources need them.
- Docstring on the class explaining the protocol purpose
- Docstring on `search()` explaining each parameter

**File 2: `src/sources/google_news.py`**

Create the GoogleNewsSource class. It must structurally satisfy the NewsSource Protocol (no inheritance needed). Key implementation details:

**Constructor:**
- Accept optional `httpx.AsyncClient` (dependency injection for Phase 6 to share connection pool)
- Accept `timeout: float = 15.0` for HTTP requests
- If no client provided, set `self._owns_client = True` and create client lazily on first search call
- Store `_client: httpx.AsyncClient | None` and `_owns_client: bool`

**Language mapping dict `_LANG_TO_HL`:**
```python
_LANG_TO_HL: dict[str, str] = {
    "en": "en-IN",  # English for India (NOT bare "en")
    "hi": "hi", "ta": "ta", "te": "te", "bn": "bn",
    "mr": "mr", "gu": "gu", "kn": "kn", "ml": "ml",
    "or": "or", "pa": "pa", "as": "as", "ur": "ur", "ne": "ne",
}
```

**URL construction (`_build_url` static/class method):**
- Format: `https://news.google.com/rss/search?q={encoded_query}&hl={hl}&gl={country}&ceid={country}:{hl}`
- Use `urllib.parse.quote_plus(query)` for proper Unicode encoding (Devanagari, Tamil, etc.)
- Look up `hl` from `_LANG_TO_HL`, fall back to bare language code if not found

**`search()` method:**
- Signature must match the Protocol exactly (including `state` and `search_term` keyword args)
- Create httpx client if none exists and `_owns_client` is True
- Wrap entire HTTP fetch + parse in try/except:
  - `httpx.HTTPStatusError` (4xx/5xx) -> log warning, return `[]`
  - `httpx.TimeoutException` -> log warning, return `[]`
  - `httpx.RequestError` (network errors) -> log warning, return `[]`
  - `Exception` (catch-all for feedparser issues, unexpected errors) -> log error, return `[]`
- On success: `response = await client.get(url, timeout=self._timeout)` then `response.raise_for_status()`
- Parse: `feed = feedparser.parse(response.text)` (sync call on string is near-instant, no executor needed)
- Convert each `feed.entries` to ArticleRef via `_entry_to_article_ref()` helper
- Skip entries that return None (missing title, link, or date)
- Log at INFO level: number of entries found, number successfully parsed
- Return the list of ArticleRef objects

**`_entry_to_article_ref()` helper function (module-level or static method):**
- Extract `title` from `entry.title` (skip if missing/empty)
- Extract `link` from `entry.link` (skip if missing/empty) -- store Google News redirect URL as-is (do NOT resolve)
- Extract source name: first try `entry.source.title`, fallback to splitting title on last ` - ` separator, fallback to `"Unknown"`
- Parse date: `entry.published_parsed` is a `time.struct_time` in UTC. Convert: `datetime(*t[:6], tzinfo=timezone.utc)`. Skip entry if no `published_parsed`.
- Return `ArticleRef(title=title, url=link, source=source_name, date=utc_dt, language=language, state=state, search_term=search_term)`
- The ArticleRef's field_validator will auto-convert UTC to IST

**`close()` async method:**
- If `_owns_client` and `_client` is not None, call `await self._client.aclose()`

**Async context manager support:**
- Implement `__aenter__` (returns self) and `__aexit__` (calls close())
- This allows `async with GoogleNewsSource() as source: ...`

**Logging:**
- Use `logging.getLogger(__name__)` at module level
- Log at DEBUG: URL being fetched
- Log at INFO: query + language + results count
- Log at WARNING: HTTP errors, timeouts, skipped entries
- Log at ERROR: unexpected exceptions

**Important anti-patterns to avoid:**
- Do NOT import or use pygooglenews
- Do NOT try to resolve/decode Google News redirect URLs
- Do NOT add googlenewsdecoder as a dependency
- Do NOT use `asyncio.run_in_executor` for feedparser.parse()
- Do NOT hardcode `when` parameter in URLs (caller's responsibility)
- Do NOT raise exceptions from search() -- always return empty list on failure
  </action>
  <verify>
Run the following Python verification script to confirm both files exist, import correctly, and GoogleNewsSource satisfies the Protocol:

```bash
cd /Users/akashyadav/Desktop/AIDMI/Github/heat-news-extraction && python -c "
from src.sources._protocol import NewsSource
from src.sources.google_news import GoogleNewsSource

# Protocol check
source = GoogleNewsSource()
assert isinstance(source, NewsSource), 'GoogleNewsSource does not satisfy NewsSource Protocol'

# Verify the language mapping has all 14 codes
from src.sources.google_news import _LANG_TO_HL
assert len(_LANG_TO_HL) == 14, f'Expected 14 language codes, got {len(_LANG_TO_HL)}'
assert _LANG_TO_HL['en'] == 'en-IN', 'English must map to en-IN for India'

print('All checks passed')
"
```
  </verify>
  <done>
- `src/sources/_protocol.py` exists with `NewsSource` Protocol class
- `src/sources/google_news.py` exists with `GoogleNewsSource` class
- `GoogleNewsSource()` passes `isinstance(source, NewsSource)` check
- Language mapping has all 14 codes with `en` -> `en-IN`
- `search()` method has correct signature matching the Protocol
- Error handling wraps all HTTP/parse operations -- search() never raises
  </done>
</task>

<task type="auto">
  <name>Task 2: Update package re-exports and run live verification</name>
  <files>
    src/sources/__init__.py
  </files>
  <action>
**Update `src/sources/__init__.py`:**

Replace the existing docstring-only content with re-exports:

```python
"""News source adapters for the heat news extraction pipeline.

This sub-package holds adapters for fetching news from various sources
including Google News RSS, NewsData.io, and GNews APIs.
"""

from ._protocol import NewsSource
from .google_news import GoogleNewsSource

__all__ = ["NewsSource", "GoogleNewsSource"]
```

**Run a live verification script** (create a temporary script, run it, then delete it):

Write a temporary `_verify_phase4.py` at the project root that does:

```python
"""Temporary verification script for Phase 4 -- delete after use."""
import asyncio
from src.sources import NewsSource, GoogleNewsSource

async def main():
    async with GoogleNewsSource(timeout=15.0) as source:
        # Verify Protocol satisfaction
        assert isinstance(source, NewsSource), "Protocol check failed"

        # Live search: English heat wave query for India
        results = await source.search(
            query="heat wave India",
            language="en",
            country="IN",
            state="India",
            search_term="heat wave",
        )

        print(f"Results: {len(results)} articles")
        for ref in results[:3]:  # Show first 3
            print(f"  - {ref.title[:80]}")
            print(f"    Source: {ref.source}, Date: {ref.date}, Lang: {ref.language}")
            print(f"    URL: {ref.url[:80]}...")

        # Verify ArticleRef fields are populated
        if results:
            r = results[0]
            assert r.title, "title is empty"
            assert r.url, "url is empty"
            assert r.source, "source is empty"
            assert r.date is not None, "date is None"
            assert r.language == "en", f"language is {r.language}, expected en"
            assert r.state == "India", f"state is {r.state}, expected India"
            assert r.search_term == "heat wave", f"search_term is {r.search_term}"
            assert str(r.date.tzinfo) == "Asia/Kolkata", f"date not in IST: {r.date.tzinfo}"
            print("\nAll ArticleRef field validations passed!")
        else:
            print("\nWARNING: No results returned (may be rate-limited or no current heat news)")
            print("This is acceptable -- the source handled empty results gracefully")

        # Test error handling: bad query should return empty list, not crash
        bad_results = await source.search(
            query="",
            language="xx",  # invalid language code will be passed through
            country="ZZ",
            state="Test",
            search_term="test",
        )
        # This may fail at ArticleRef validation (language "xx" not in allowed pattern)
        # but search() should not raise -- it should return [] or skip invalid entries
        print(f"\nBad query test: returned {len(bad_results)} results (expected 0 or few)")
        print("Error handling verified -- no crash")

asyncio.run(main())
```

Run the script: `python _verify_phase4.py`

Then delete the script: `rm _verify_phase4.py`

The verification should show:
1. Protocol check passes
2. Live results are returned (or graceful empty list if rate-limited)
3. ArticleRef fields are correctly populated
4. Bad queries don't crash
  </action>
  <verify>
```bash
cd /Users/akashyadav/Desktop/AIDMI/Github/heat-news-extraction && python -c "
from src.sources import NewsSource, GoogleNewsSource
assert 'NewsSource' in dir()
assert 'GoogleNewsSource' in dir()

# Verify clean imports from package level
import src.sources
assert hasattr(src.sources, 'NewsSource')
assert hasattr(src.sources, 'GoogleNewsSource')
print('Package re-exports verified')
"
```
  </verify>
  <done>
- `src/sources/__init__.py` re-exports `NewsSource` and `GoogleNewsSource`
- `from src.sources import NewsSource, GoogleNewsSource` works
- Live verification script ran successfully (results returned or graceful empty list)
- Bad query/error handling verified -- no crashes
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.sources import NewsSource, GoogleNewsSource; assert isinstance(GoogleNewsSource(), NewsSource)"` -- Protocol check
2. `python -c "from src.sources.google_news import _LANG_TO_HL; assert len(_LANG_TO_HL) == 14 and _LANG_TO_HL['en'] == 'en-IN'"` -- Language mapping
3. Live search test returns ArticleRef objects with populated fields (or empty list if rate-limited)
4. Error handling: search() with invalid parameters returns empty list, does not raise
</verification>

<success_criteria>
- NewsSource Protocol exists at src/sources/_protocol.py with async search() method
- GoogleNewsSource exists at src/sources/google_news.py and satisfies the Protocol
- GoogleNewsSource.search() fetches real Google News RSS results and parses them into ArticleRef objects
- All 14 language codes are mapped (en -> en-IN for India)
- HTTP errors, timeouts, empty results handled gracefully (returns [], never raises)
- Package re-exports work: from src.sources import NewsSource, GoogleNewsSource
</success_criteria>

<output>
After completion, create `.planning/phases/04-google-news-rss-source/04-01-SUMMARY.md`
</output>
