---
phase: 02-data-models-and-geographic-data
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/__init__.py
  - src/data/india_geo.json
  - src/data/geo_loader.py
autonomous: true

must_haves:
  truths:
    - "A structured JSON file contains all 36 states/UTs with their districts and language mappings"
    - "The geographic data includes approximately 770+ districts across all 36 states/UTs"
    - "Each state/UT maps to its relevant languages from the supported 14 language codes"
    - "Tamil Nadu maps to ['ta', 'en'], Delhi maps to ['hi', 'en', 'ur', 'pa'], etc."
    - "Geographic data loads reliably regardless of working directory (uses Path(__file__).parent)"
    - "Loading the data validates it against Pydantic models -- malformed data is caught at startup"
  artifacts:
    - path: "src/data/india_geo.json"
      provides: "Master geographic data file with all 36 states/UTs, ~770 districts, language mappings"
      contains: "tamil-nadu"
    - path: "src/data/geo_loader.py"
      provides: "Functions to load, validate, and query geographic data"
      exports: ["load_geo_data", "get_all_states", "get_all_uts", "get_all_regions", "get_languages_for_region"]
    - path: "src/data/__init__.py"
      provides: "Package marker and re-exports for src.data"
      exports: ["load_geo_data", "get_all_states", "get_all_uts", "get_all_regions", "get_languages_for_region"]
  key_links:
    - from: "src/data/geo_loader.py"
      to: "src/data/india_geo.json"
      via: "Path(__file__).parent / 'india_geo.json'"
      pattern: "Path.*__file__.*india_geo"
    - from: "src/data/geo_loader.py"
      to: "pydantic"
      via: "model_validate for JSON validation"
      pattern: "model_validate"
    - from: "src/data/__init__.py"
      to: "src/data/geo_loader.py"
      via: "re-exports"
      pattern: "from .geo_loader import"
---

<objective>
Create the complete geographic master data file (all 36 Indian states/UTs with ~770 districts and language mappings) and the Pydantic-validated loader that makes this data available to the pipeline.

Purpose: Every downstream phase (query generation, output organization, language selection) needs to know which states/UTs exist, what districts they contain, and which languages to use for each region.
Output: src/data/india_geo.json (master data), src/data/geo_loader.py (loader with Pydantic validation), src/data/__init__.py (re-exports).
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-models-and-geographic-data/02-RESEARCH.md
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create india_geo.json master data file with all 36 states/UTs, districts, and language mappings</name>
  <files>src/data/__init__.py, src/data/india_geo.json</files>
  <action>
First create `src/data/__init__.py` as a package marker (will be updated with re-exports in Task 2).

Then create `src/data/india_geo.json` by assembling data from TWO verified sources:

**Source 1 -- District data:** Use the sab99r/Indian-States-And-Districts GitHub dataset as the baseline. Fetch it from `https://raw.githubusercontent.com/sab99r/Indian-States-And-Districts/master/states-and-districts.json`. This provides 35 entries with 768 districts.

**Source 2 -- Language mappings:** Use the monsoon pipeline's `language_map.py` at `/Users/akashyadav/Desktop/AIDMI/Github/monsoon-news-extraction/language_map.py` for state-to-language associations. The `multilingual_mapping` dict has all 36 states/UTs.

**Assembly steps:**

1. Start with the sab99r district data (35 entries, 768 districts).

2. Fix structural issues with the sab99r data:
   - **Merge "Dadra and Nagar Haveli (UT)" and "Daman and Diu (UT)"** into a single entry: "Dadra and Nagar Haveli and Daman and Diu" with slug "dadra-and-nagar-haveli-and-daman-and-diu" and type "ut". Combine their district lists.
   - **Add "Andaman and Nicobar Islands"** (missing from sab99r) with type "ut", slug "andaman-and-nicobar-islands", and districts: Nicobar, North and Middle Andaman, South Andaman.
   - **Add "Ladakh"** (missing from sab99r, carved from J&K in 2019) with type "ut", slug "ladakh", and districts: Leh, Kargil.

3. For each state/UT, add the `languages` array from the monsoon pipeline's `multilingual_mapping`:
   - **Critical:** Replace "mni" (Meitei/Manipuri) with `["en", "hi"]` for Manipur. The supported 14 languages do not include mni.
   - **Critical:** Replace "lus" (Mizo) with `["en", "hi"]` for Mizoram. The supported 14 languages do not include lus.
   - All other language codes from the monsoon pipeline are within the 14 supported: en, hi, ta, te, bn, mr, gu, kn, ml, or, pa, as, ur, ne.

4. Add the `type` field: "state" or "ut" for each entry.
   - **28 states:** Andhra Pradesh, Arunachal Pradesh, Assam, Bihar, Chhattisgarh, Goa, Gujarat, Haryana, Himachal Pradesh, Jharkhand, Karnataka, Kerala, Madhya Pradesh, Maharashtra, Manipur, Meghalaya, Mizoram, Nagaland, Odisha, Punjab, Rajasthan, Sikkim, Tamil Nadu, Telangana, Tripura, Uttar Pradesh, Uttarakhand, West Bengal.
   - **8 union territories:** Andaman and Nicobar Islands, Chandigarh, Dadra and Nagar Haveli and Daman and Diu, Delhi, Jammu and Kashmir, Ladakh, Lakshadweep, Puducherry.

5. Generate `slug` for each state/UT and district: lowercase, replace spaces with hyphens, remove parenthetical suffixes like "(UT)" or "(NCT)". Example: "Tamil Nadu" -> "tamil-nadu", "North and Middle Andaman" -> "north-and-middle-andaman".

**Final JSON format:**
```json
{
  "states": [
    {
      "name": "Andhra Pradesh",
      "slug": "andhra-pradesh",
      "type": "state",
      "languages": ["te", "en"],
      "districts": [
        {"name": "Anantapur", "slug": "anantapur"},
        ...
      ]
    },
    ...
  ]
}
```

**Validation requirements:**
- Exactly 36 entries in the "states" array (28 states + 8 UTs)
- Every entry has: name (str), slug (str), type ("state"|"ut"), languages (list of str from the 14 supported codes), districts (list of {name, slug})
- Total districts should be approximately 770+ (768 from sab99r + adjustments for merges and additions)
- Every language code in every languages array must be one of: en, hi, ta, te, bn, mr, gu, kn, ml, or, pa, as, ur, ne (NO "mni", NO "lus")

**Important:** This is a large data file. Accuracy matters more than speed. Double-check that:
- All 36 states/UTs from the monsoon pipeline's multilingual_mapping are present
- The slug format matches the monsoon pipeline's convention (kebab-case)
- Language arrays match the monsoon pipeline EXCEPT for the mni/lus replacements
  </action>
  <verify>
Run a Python script to validate the JSON:
```
python -c "
import json
from pathlib import Path

data = json.loads(Path('src/data/india_geo.json').read_text())
states = data['states']

# Check count
assert len(states) == 36, f'Expected 36 entries, got {len(states)}'

# Check all have required fields
for s in states:
    assert 'name' in s, f'Missing name'
    assert 'slug' in s, f'Missing slug for {s.get(\"name\")}'
    assert 'type' in s and s['type'] in ('state', 'ut'), f'Bad type for {s[\"name\"]}: {s.get(\"type\")}'
    assert 'languages' in s and len(s['languages']) > 0, f'No languages for {s[\"name\"]}'
    assert 'districts' in s and len(s['districts']) > 0, f'No districts for {s[\"name\"]}'
    for lang in s['languages']:
        assert lang in ('en','hi','ta','te','bn','mr','gu','kn','ml','or','pa','as','ur','ne'), f'Invalid lang {lang} for {s[\"name\"]}'
    for d in s['districts']:
        assert 'name' in d and 'slug' in d, f'District missing fields in {s[\"name\"]}'

# Count types
state_count = sum(1 for s in states if s['type'] == 'state')
ut_count = sum(1 for s in states if s['type'] == 'ut')
assert state_count == 28, f'Expected 28 states, got {state_count}'
assert ut_count == 8, f'Expected 8 UTs, got {ut_count}'

# Total districts
total = sum(len(s['districts']) for s in states)
print(f'Total entries: {len(states)} ({state_count} states, {ut_count} UTs)')
print(f'Total districts: {total}')
assert total >= 740, f'Expected 740+ districts, got {total}'

# Check specific mappings
slugs = {s['slug']: s for s in states}
assert 'tamil-nadu' in slugs, 'Missing Tamil Nadu'
assert slugs['tamil-nadu']['languages'] == ['ta', 'en'], f'Wrong langs for TN: {slugs[\"tamil-nadu\"][\"languages\"]}'
assert 'delhi' in slugs, 'Missing Delhi'
assert slugs['delhi']['languages'] == ['hi', 'en', 'ur', 'pa'], f'Wrong langs for Delhi: {slugs[\"delhi\"][\"languages\"]}'
assert 'manipur' in slugs, 'Missing Manipur'
assert 'mni' not in slugs['manipur']['languages'], 'mni should be replaced in Manipur'
assert 'mizoram' in slugs, 'Missing Mizoram'
assert 'lus' not in slugs['mizoram']['languages'], 'lus should be replaced in Mizoram'
assert 'ladakh' in slugs, 'Missing Ladakh'
assert 'andaman-and-nicobar-islands' in slugs, 'Missing A&N Islands'
assert 'dadra-and-nagar-haveli-and-daman-and-diu' in slugs, 'Missing merged DNH+DD'

# No mni or lus anywhere
for s in states:
    for lang in s['languages']:
        assert lang not in ('mni', 'lus'), f'Unsupported lang {lang} in {s[\"name\"]}'

print('ALL CHECKS PASSED')
"
```
  </verify>
  <done>src/data/india_geo.json exists with exactly 36 entries (28 states + 8 UTs), 740+ districts, language mappings from the 14 supported codes only (no mni/lus), and all states/UTs from the monsoon pipeline represented.</done>
</task>

<task type="auto">
  <name>Task 2: Create geo_loader.py with Pydantic validation and query functions</name>
  <files>src/data/geo_loader.py, src/data/__init__.py</files>
  <action>
Create `src/data/geo_loader.py` with the following:

**Pydantic models for geographic data validation:**

```python
from pydantic import BaseModel, ConfigDict, Field, field_validator
```

- `District(BaseModel)` -- frozen, fields: `name: str`, `slug: str`
- `StateUT(BaseModel)` -- frozen, fields: `name: str`, `slug: str`, `type: str` (literal "state" or "ut" -- use `Literal["state", "ut"]`), `languages: list[str]` (Field with min_length=1), `districts: list[District]` (Field with min_length=1)
- `GeoData(BaseModel)` -- frozen, field: `states: list[StateUT]`
- Add a `field_validator("languages")` on StateUT that checks all language codes are within the supported 14 codes: `{"en", "hi", "ta", "te", "bn", "mr", "gu", "kn", "ml", "or", "pa", "as", "ur", "ne"}`. Raise ValueError if any unsupported code found.

**Loader function:**

```python
from functools import lru_cache
from pathlib import Path
import json

_DATA_DIR = Path(__file__).parent

@lru_cache(maxsize=1)
def load_geo_data() -> GeoData:
    """Load and validate all geographic data from the master JSON file.

    Cached: only reads disk and validates once per process.
    Raises FileNotFoundError if india_geo.json is missing.
    Raises pydantic.ValidationError if data is malformed.
    """
    data_path = _DATA_DIR / "india_geo.json"
    raw = json.loads(data_path.read_text(encoding="utf-8"))
    return GeoData.model_validate(raw)
```

**Query functions (all delegate to load_geo_data):**

- `get_all_regions() -> list[StateUT]` -- returns all 36 entries
- `get_all_states() -> list[StateUT]` -- returns entries where type == "state" (28)
- `get_all_uts() -> list[StateUT]` -- returns entries where type == "ut" (8)
- `get_region_by_slug(slug: str) -> StateUT | None` -- finds by slug, returns None if not found
- `get_languages_for_region(slug: str) -> list[str]` -- returns language codes for a region, defaults to ["en"] if slug not found
- `get_districts_for_region(slug: str) -> list[District]` -- returns district list for a region, empty list if not found

**Update `src/data/__init__.py`** to re-export:
```python
"""Geographic and reference data for the heat news extraction pipeline."""

from .geo_loader import (
    District,
    GeoData,
    StateUT,
    get_all_regions,
    get_all_states,
    get_all_uts,
    get_districts_for_region,
    get_languages_for_region,
    get_region_by_slug,
    load_geo_data,
)

__all__ = [
    "District",
    "GeoData",
    "StateUT",
    "get_all_regions",
    "get_all_states",
    "get_all_uts",
    "get_districts_for_region",
    "get_languages_for_region",
    "get_region_by_slug",
    "load_geo_data",
]
```

Use `Literal["state", "ut"]` from `typing` for the type field, not a plain str. Use `ConfigDict(frozen=True)` on all models. Do NOT use pytz. Do NOT use any external dependencies beyond pydantic and stdlib.
  </action>
  <verify>
Run: `python -c "
from src.data import (
    load_geo_data, get_all_states, get_all_uts, get_all_regions,
    get_languages_for_region, get_region_by_slug, get_districts_for_region,
    StateUT, District, GeoData
)

# Test 1: Load validates successfully
geo = load_geo_data()
print(f'Loaded {len(geo.states)} regions')
assert len(geo.states) == 36

# Test 2: States vs UTs
states = get_all_states()
uts = get_all_uts()
assert len(states) == 28, f'Expected 28 states, got {len(states)}'
assert len(uts) == 8, f'Expected 8 UTs, got {len(uts)}'
print(f'States: {len(states)}, UTs: {len(uts)}')

# Test 3: Language lookup
tn_langs = get_languages_for_region('tamil-nadu')
assert tn_langs == ['ta', 'en'], f'TN langs: {tn_langs}'
delhi_langs = get_languages_for_region('delhi')
assert delhi_langs == ['hi', 'en', 'ur', 'pa'], f'Delhi langs: {delhi_langs}'
unknown_langs = get_languages_for_region('nonexistent')
assert unknown_langs == ['en'], f'Unknown region should return [\"en\"]'
print('Language lookups correct')

# Test 4: Region lookup
tn = get_region_by_slug('tamil-nadu')
assert tn is not None
assert tn.name == 'Tamil Nadu'
assert tn.type == 'state'
missing = get_region_by_slug('nonexistent')
assert missing is None
print('Region lookups correct')

# Test 5: District lookup
tn_districts = get_districts_for_region('tamil-nadu')
assert len(tn_districts) > 0
assert all(isinstance(d, District) for d in tn_districts)
print(f'Tamil Nadu districts: {len(tn_districts)}')

# Test 6: Total districts
total = sum(len(r.districts) for r in get_all_regions())
print(f'Total districts: {total}')
assert total >= 740

# Test 7: Frozen models
try:
    tn.name = 'Changed'
    print('FAIL: StateUT should be frozen')
except Exception:
    print('PASS: StateUT is frozen')

# Test 8: Caching works (second call is same object)
geo2 = load_geo_data()
assert geo is geo2, 'lru_cache should return same object'
print('PASS: Caching works')

print('ALL TESTS PASSED')
"

Also run: `python main.py` to confirm no regressions.
  </verify>
  <done>geo_loader.py loads and validates india_geo.json via Pydantic models. All query functions work: get_all_states (28), get_all_uts (8), get_all_regions (36), language lookups match monsoon pipeline mappings (with mni/lus replaced), district lookups return correct data. Models are frozen. Data is cached via lru_cache. python main.py still runs cleanly.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.data import load_geo_data; g = load_geo_data(); print(len(g.states))"` prints 36
2. `python -c "from src.data import get_all_states, get_all_uts; print(len(get_all_states()), len(get_all_uts()))"` prints "28 8"
3. `python -c "from src.data import get_languages_for_region; print(get_languages_for_region('tamil-nadu'))"` prints "['ta', 'en']"
4. `python -c "from src.data import get_languages_for_region; print(get_languages_for_region('manipur'))"` does NOT contain "mni"
5. `python -c "from src.data import get_region_by_slug; r = get_region_by_slug('ladakh'); print(r.name, len(r.districts))"` prints "Ladakh 2"
6. india_geo.json is valid JSON with 36 entries, 740+ districts, only the 14 supported language codes
7. `python main.py` still runs cleanly
</verification>

<success_criteria>
- india_geo.json contains all 36 states/UTs (28 states + 8 UTs) with districts and language mappings (INFR-05)
- Language codes constrained to the 14 supported codes -- no mni, no lus
- Language mappings match monsoon pipeline except for mni/lus replacements
- Data loads via Path(__file__).parent -- works from any working directory
- Pydantic models validate the JSON at load time -- malformed data raises clear errors
- Query functions provide convenient access by slug, type, language
- lru_cache ensures the file is read only once per process
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-models-and-geographic-data/02-02-SUMMARY.md`
</output>
