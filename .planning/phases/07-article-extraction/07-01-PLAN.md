---
phase: 07-article-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/extraction/_resolver.py
  - src/extraction/_extractor.py
  - src/extraction/__init__.py
autonomous: true

must_haves:
  truths:
    - "Given a list of ArticleRef objects, the extractor produces Article objects with full_text populated for the majority"
    - "Google News redirect URLs are resolved to actual article URLs before extraction"
    - "Indian language scripts (Devanagari, Tamil, Telugu, Bengali, etc.) are preserved without mojibake in extracted text"
    - "Failed extractions (timeouts, blocked sites, paywalls) are logged with URL and reason but do not halt the pipeline"
    - "Extraction runs asynchronously with bounded concurrency to prevent resource exhaustion"
  artifacts:
    - path: "src/extraction/_resolver.py"
      provides: "Google News URL resolution (redirect following + batchexecute fallback)"
      contains: "resolve_url"
    - path: "src/extraction/_extractor.py"
      provides: "Trafilatura-based article extraction with async bridge and batch processing"
      contains: "extract_articles"
    - path: "src/extraction/__init__.py"
      provides: "Package re-exports for extraction module"
      contains: "extract_articles"
  key_links:
    - from: "src/extraction/_extractor.py"
      to: "src/extraction/_resolver.py"
      via: "resolve_url import"
      pattern: "from ._resolver import resolve_url"
    - from: "src/extraction/_extractor.py"
      to: "src/models/article.py"
      via: "ArticleRef to Article conversion"
      pattern: "Article\\(\\*\\*ref\\.model_dump\\(\\)"
    - from: "src/extraction/_extractor.py"
      to: "trafilatura"
      via: "asyncio.to_thread bridge"
      pattern: "asyncio\\.to_thread.*trafilatura\\.extract"
---

<objective>
Implement full article text extraction from collected URLs using trafilatura, including Google News URL resolution and async batch processing with bounded concurrency.

Purpose: Phase 6 produces ArticleRef objects with URLs (many of which are Google News redirects). This plan resolves those URLs to actual article pages, extracts full article text using trafilatura, and converts ArticleRef objects into Article objects with full_text populated. This is the bridge between "we found articles" and "we have article content."

Output: `src/extraction/_resolver.py`, `src/extraction/_extractor.py`, updated `src/extraction/__init__.py`
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-article-extraction/07-RESEARCH.md
@src/models/article.py
@src/extraction/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Google News URL resolver and trafilatura extractor</name>
  <files>
    src/extraction/_resolver.py
    src/extraction/_extractor.py
    src/extraction/__init__.py
  </files>
  <action>
Create `src/extraction/_resolver.py` -- Google News URL resolver:

1. Create an async function `resolve_url(url: str, client: httpx.AsyncClient) -> str` that:
   - Returns `url` unchanged if `"news.google.com"` is NOT in the URL (non-Google URLs pass through)
   - For Google News URLs, uses a two-strategy resolution approach:
     - **Strategy 1:** HTTP GET with `follow_redirects=True`, timeout 10s. If the final URL is NOT `news.google.com`, return it (resolved).
     - **Strategy 2 (fallback):** batchexecute endpoint decoding for URLs that don't redirect. Steps:
       a. Extract the article ID from the URL path (last segment after `/`)
       b. Fetch `https://news.google.com/rss/articles/{article_id}` to get the HTML page
       c. Parse HTML with `lxml.html.fromstring()` and extract `data-n-a-sg` (signature) and `data-n-a-ts` (timestamp) attributes from the first `c-wiz > div` element (use XPath: `tree.xpath("//c-wiz/div")` to avoid cssselect import issues)
       d. POST to `https://news.google.com/_/DotsSplashUi/data/batchexecute` with the encoded payload containing the article ID, signature, and timestamp
       e. Parse the batchexecute response: skip first `\n\n` boundary, parse JSON, extract the decoded URL from `json.loads(data[0][2])[1]`
   - Returns original URL if all strategies fail (graceful degradation)
   - Wraps ALL HTTP calls in try/except -- never raises, always returns a URL string
   - Uses `logging.getLogger(__name__)` for debug/warning logs

2. Create `src/extraction/_extractor.py` -- trafilatura wrapper with async bridge:

   a. `async def _fetch_html(url: str, client: httpx.AsyncClient) -> str | None`:
      - Fetches URL with `follow_redirects=True`, `timeout=httpx.Timeout(15.0, connect=5.0)`
      - Calls `response.raise_for_status()`
      - Returns `response.text` (httpx decodes using charset header -- this prevents Indian script mojibake per research)
      - On `httpx.TimeoutException`: log warning with URL, return None
      - On `httpx.HTTPStatusError`: log warning with status code and URL, return None
      - On any other exception: log error with exc_info, return None

   b. `async def _extract_text(html: str, url: str) -> str | None`:
      - Runs `trafilatura.extract()` via `asyncio.to_thread()` to avoid blocking the event loop
      - Parameters: `favor_recall=True`, `include_comments=False`, `include_tables=True`, `deduplicate=True`, `url=url`
      - Returns the extracted text string or None
      - Wraps in try/except -- log and return None on any trafilatura error

   c. `async def extract_article(ref: ArticleRef, client: httpx.AsyncClient) -> Article`:
      - Step 1: Resolve URL via `resolve_url(ref.url, client)` from `_resolver`
      - Step 2: Fetch HTML via `_fetch_html(actual_url, client)`
      - Step 3: Extract text via `_extract_text(html, actual_url)` (skip if html is None)
      - Step 4: Create Article via `Article(**ref.model_dump(), full_text=text, relevance_score=0.0)`
      - Log extraction outcome: chars extracted on success, "no text" on None, "fetch failed" if HTML was None
      - On ANY exception: log error with exc_info, return `Article(**ref.model_dump(), full_text=None, relevance_score=0.0)`
      - NEVER raises -- requirement EXTR-03

   d. `async def extract_articles(refs: list[ArticleRef], max_concurrent: int = 10) -> list[Article]`:
      - This is the public batch API consumed by the pipeline
      - Creates a shared `httpx.AsyncClient` with `follow_redirects=True` and `timeout=httpx.Timeout(15.0, connect=5.0)`
      - Uses `asyncio.Semaphore(max_concurrent)` to bound concurrency
      - Creates a wrapper `_extract_one(ref)` that acquires semaphore before calling `extract_article`
      - Uses `asyncio.gather(*tasks, return_exceptions=False)` -- since extract_article never raises, no exceptions expected
      - Logs batch summary: total refs, successful extractions (full_text is not None), failed extractions
      - Returns `list[Article]`

3. Update `src/extraction/__init__.py`:
   - Re-export `extract_articles` and `extract_article` from `_extractor`
   - Re-export `resolve_url` from `_resolver`
   - Keep the existing module docstring, update it to reflect the implemented functionality

Important implementation details:
- Use `from __future__ import annotations` in all new files (codebase convention)
- Use `logging.getLogger(__name__)` (codebase convention)
- `Article(**ref.model_dump(), full_text=text, relevance_score=0.0)` is the correct pattern for ArticleRef-to-Article conversion (frozen models, cannot mutate)
- httpx `response.text` handles charset decoding -- pass the decoded string (not bytes) to trafilatura to prevent Indian script encoding issues
- Do NOT use `trafilatura.fetch_url()` -- it is synchronous and uses urllib3 internally (anti-pattern from research)
- Do NOT use unlimited concurrency -- bounded semaphore is essential (research pitfall 3)
- The `relevance_score=0.0` default is correct -- Phase 8 will set the actual score
  </action>
  <verify>
Run: `python -c "from src.extraction import extract_articles, extract_article, resolve_url; print('imports ok')"` -- should print "imports ok" without errors.

Run: `python -c "
import asyncio, httpx
from src.extraction._resolver import resolve_url

async def test():
    async with httpx.AsyncClient() as client:
        # Non-Google URL should pass through unchanged
        result = await resolve_url('https://example.com/article', client)
        assert result == 'https://example.com/article', f'Expected passthrough, got {result}'
        print('URL passthrough: OK')

asyncio.run(test())
"` -- should print "URL passthrough: OK".

Run: `python -c "
from src.extraction._extractor import extract_articles, extract_article
import inspect
sig = inspect.signature(extract_articles)
params = list(sig.parameters.keys())
assert 'refs' in params, 'Missing refs param'
assert 'max_concurrent' in params, 'Missing max_concurrent param'
print('extract_articles signature: OK')

sig2 = inspect.signature(extract_article)
params2 = list(sig2.parameters.keys())
assert 'ref' in params2, 'Missing ref param'
assert 'client' in params2, 'Missing client param'
print('extract_article signature: OK')
"` -- should confirm function signatures.
  </verify>
  <done>
- `resolve_url()` resolves Google News redirect URLs via HTTP redirect following + batchexecute fallback, passes non-Google URLs through unchanged
- `extract_article()` takes an ArticleRef and produces an Article with full_text populated (or None on failure), never raises
- `extract_articles()` batch-processes a list of ArticleRef with bounded concurrency (semaphore), returns list of Article
- All extraction failures are logged with URL and reason (EXTR-03)
- Indian script text is preserved via httpx charset decoding + trafilatura (EXTR-02)
- `src/extraction/__init__.py` re-exports `extract_articles`, `extract_article`, `resolve_url`
  </done>
</task>

</tasks>

<verification>
1. All three files exist: `src/extraction/_resolver.py`, `src/extraction/_extractor.py`, `src/extraction/__init__.py`
2. Package imports work: `from src.extraction import extract_articles, extract_article, resolve_url`
3. `resolve_url` passes non-Google URLs through unchanged
4. `extract_article` returns an Article (never raises) even with invalid URLs
5. `extract_articles` accepts a list of ArticleRef and returns a list of Article
6. No new dependencies added to requirements.txt (everything is already installed)
</verification>

<success_criteria>
- EXTR-01: `extract_articles()` uses trafilatura to extract full article text from URLs, producing Article objects with full_text field
- EXTR-02: Indian scripts preserved by using `response.text` (httpx charset decoding) + trafilatura's charset-normalizer
- EXTR-03: Every extraction call wrapped in try/except; failures logged with URL and reason; pipeline continues
- Integration: `extract_articles(refs)` takes the flat `list[ArticleRef]` output of `QueryExecutor.run_collection()` and returns `list[Article]`
</success_criteria>

<output>
After completion, create `.planning/phases/07-article-extraction/07-01-SUMMARY.md`
</output>
