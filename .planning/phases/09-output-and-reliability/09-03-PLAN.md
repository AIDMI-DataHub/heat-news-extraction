---
phase: 09-output-and-reliability
plan: 03
type: execute
wave: 2
depends_on: ["09-01", "09-02"]
files_modified:
  - src/query/_executor.py
  - main.py
autonomous: true

must_haves:
  truths:
    - "Pipeline skips already-completed queries by checking the checkpoint store before execution"
    - "Checkpoint is saved after each individual query completion for maximum recovery granularity"
    - "Pipeline produces organized JSON/CSV output grouped by date and state after dedup+filter"
    - "Pipeline outputs a _metadata.json with collection timestamp, sources queried, query terms, and article counts"
    - "State-level queries complete before district-level queries (RELI-05, already implemented -- preserved)"
    - "main.py wires the complete pipeline: sources -> schedulers (with circuit breakers) -> executor (with checkpoint) -> extraction -> dedup -> output"
  artifacts:
    - path: "src/query/_executor.py"
      provides: "QueryExecutor with CheckpointStore integration for skip/save per query"
      min_lines: 100
    - path: "main.py"
      provides: "Complete pipeline orchestration wiring all stages end-to-end"
      min_lines: 60
  key_links:
    - from: "src/query/_executor.py"
      to: "src/reliability/_checkpoint.py"
      via: "CheckpointStore.is_completed() and mark_completed() + save() in _execute_query_list"
      pattern: "checkpoint.*is_completed|mark_completed|save"
    - from: "main.py"
      to: "src/output/_writers.py"
      via: "write_collection_output() after dedup+filter stage"
      pattern: "write_collection_output"
    - from: "main.py"
      to: "src/reliability/_circuit_breaker.py"
      via: "CircuitBreaker instances passed to factory schedulers"
      pattern: "CircuitBreaker"
    - from: "main.py"
      to: "src/query/_executor.py"
      via: "QueryExecutor.run_collection() returns ArticleRefs"
      pattern: "run_collection"
---

<objective>
Integrate checkpoint/resume into the QueryExecutor and wire the complete pipeline in main.py -- sources, schedulers with circuit breakers, executor with checkpoint, extraction, dedup/filter, and output.

Purpose: This plan completes Phase 9 by connecting all reliability and output primitives (from plans 01 and 02) into the actual pipeline flow. After this, `python main.py` runs the full end-to-end pipeline with crash recovery, circuit breakers, retry, and organized output (RELI-01, RELI-02, RELI-05, OUTP-01 through OUTP-04).

Output: Modified QueryExecutor with checkpoint integration, complete main.py pipeline orchestration.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-output-and-reliability/09-01-SUMMARY.md
@.planning/phases/09-output-and-reliability/09-02-SUMMARY.md
@src/query/_executor.py
@src/query/_scheduler.py
@src/query/_generator.py
@src/extraction/_extractor.py
@src/dedup/__init__.py
@main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate CheckpointStore into QueryExecutor</name>
  <files>src/query/_executor.py</files>
  <action>
Modify `QueryExecutor` to accept and use a `CheckpointStore` for checkpoint/resume:

1. Add optional `checkpoint: CheckpointStore | None = None` parameter to `__init__`. Store as `self._checkpoint`. Import `CheckpointStore` under `TYPE_CHECKING` guard (same pattern as NewsSource).

2. Modify `_execute_query_list()` to integrate checkpoint skip and save:

```python
async def _execute_query_list(
    self, scheduler: SourceScheduler, queries: list[Query]
) -> list[QueryResult]:
    results: list[QueryResult] = []
    skipped_checkpoint = 0

    for query in queries:
        # Skip if already completed in a previous run
        if self._checkpoint is not None and self._checkpoint.is_completed(query):
            skipped_checkpoint += 1
            continue

        result = await scheduler.execute(query)
        results.append(result)

        # Mark completed and save checkpoint after each query
        if self._checkpoint is not None:
            await self._checkpoint.mark_completed(query)
            await self._checkpoint.save()

        # Break early if budget exhausted
        budget = scheduler.remaining_budget
        if budget is not None and budget <= 0:
            logger.info(
                "%s: budget exhausted after %d/%d queries, stopping",
                scheduler.name,
                len(results),
                len(queries),
            )
            break

    if skipped_checkpoint > 0:
        logger.info(
            "%s: skipped %d queries from checkpoint",
            scheduler.name,
            skipped_checkpoint,
        )

    return results
```

3. Preserve the existing `run_collection()` flow exactly. The checkpoint integration is entirely within `_execute_query_list()` -- no changes to the hierarchical state-then-district logic (RELI-05 already implemented).

4. Add a `@property` method `checkpoint -> CheckpointStore | None` for external access (main.py may need it for cleanup).

Note: Checkpoint saves happen in the sequential `_execute_query_list` loop (Pitfall 2 from research: "saves should happen in the sequential orchestration layer, not inside concurrent tasks"). Since each source has its own sequential query list, and sources run concurrently, the asyncio.Lock is NOT needed -- each checkpoint.save() is called from a single coroutine path per source. However, multiple sources could call save() near-simultaneously. Since CheckpointStore._completed is a set (thread-safe for GIL-protected adds) and save() writes the complete set atomically each time, the last write wins and includes all completed queries. This is correct because each save writes the full set.
  </action>
  <verify>
Run: `python -c "from src.query._executor import QueryExecutor; print('QueryExecutor imports OK')"` -- must succeed.
Run: `grep -c 'checkpoint' src/query/_executor.py` -- should have multiple occurrences.
Run: `grep -c 'is_completed\|mark_completed' src/query/_executor.py` -- should show both methods used.
  </verify>
  <done>
QueryExecutor accepts optional CheckpointStore, skips already-completed queries in _execute_query_list, and saves checkpoint after each individual query completion. Hierarchical state-then-district flow is preserved unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire complete pipeline in main.py</name>
  <files>main.py</files>
  <action>
Replace the placeholder main.py with a complete pipeline orchestration. The pipeline stages:

1. **Setup and configuration:**
   - Import all pipeline components: sources, schedulers, query generator/executor, extraction, dedup, output, reliability
   - Read API keys from environment variables: `NEWSDATA_API_KEY`, `GNEWS_API_KEY` (using `os.environ.get()`, None if not set -- sources degrade gracefully)
   - Set up logging: `logging.basicConfig(level=logging.INFO, format="%(asctime)s %(name)s %(levelname)s %(message)s")`
   - Determine output directory: `Path("output") / datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d")`
   - Create CheckpointStore at `output_dir / ".checkpoint.json"`

2. **Source and scheduler construction:**
   - Create source instances: `GoogleNewsSource()`, `NewsDataSource(api_key=newsdata_key)`, `GNewsSource(api_key=gnews_key)`
   - Create one `CircuitBreaker` per source: `CircuitBreaker(name="google_news")`, `CircuitBreaker(name="newsdata")`, `CircuitBreaker(name="gnews")`
   - Create schedulers via factory functions with circuit breakers: `create_google_scheduler(google_source, circuit_breaker=google_cb)`, etc.
   - Create `QueryGenerator()` and `QueryExecutor(schedulers={"google": ..., "newsdata": ..., "gnews": ...}, generator=generator, checkpoint=checkpoint)`

3. **Load checkpoint:**
   - `await checkpoint.load()` -- loads previously completed queries if any
   - Log how many queries were already completed: `logger.info("Checkpoint loaded: %d queries already completed", checkpoint.completed_count)`

4. **Stage 1 -- Query collection:**
   - `refs: list[ArticleRef] = await executor.run_collection()`
   - Log: articles_found count

5. **Stage 2 -- Extraction:**
   - `from src.extraction import extract_articles`
   - `articles: list[Article] = await extract_articles(refs)`
   - Log: articles_extracted count

6. **Stage 3 -- Dedup and filter:**
   - `from src.dedup import deduplicate_and_filter`
   - `filtered: list[Article] = deduplicate_and_filter(articles)`
   - Log: articles_filtered count

7. **Stage 4 -- Output:**
   - Build `CollectionMetadata` with:
     - `collection_timestamp=datetime.now(ZoneInfo("Asia/Kolkata"))`
     - `sources_queried=["google_news", "newsdata", "gnews"]`
     - `query_terms_used=sorted(set(ref.search_term for ref in refs))` (unique search terms)
     - `counts={"articles_found": len(refs), "articles_extracted": sum(1 for a in articles if a.full_text is not None), "articles_filtered": len(filtered)}`
   - `result = await write_collection_output(filtered, output_dir, metadata)`
   - Log the paths written

8. **Cleanup:**
   - Close all source instances: `await google_source.close()`, etc.
   - Delete checkpoint file on successful completion: `checkpoint_path.unlink(missing_ok=True)`
   - Log: "Pipeline complete"

9. **Error handling:**
   - Wrap the entire pipeline in try/except/finally. In finally, close sources. On unhandled exception, log error but do NOT delete checkpoint (so next run can resume).

Keep the `if __name__ == "__main__": asyncio.run(main())` entry point.

Important: Use `async with` for source instances where possible. If the sources support async context manager (they do), use a single `async with` block or manual close in finally.
  </action>
  <verify>
Run: `python -c "import main; print('main.py imports OK')"` -- must succeed without errors (it should NOT run the pipeline, just import).
Run: `grep -c 'CircuitBreaker' main.py` -- should show multiple occurrences.
Run: `grep -c 'CheckpointStore' main.py` -- should show occurrences.
Run: `grep -c 'write_collection_output' main.py` -- should show at least 1.
Run: `grep -c 'deduplicate_and_filter' main.py` -- should show at least 1.
Run: `grep -c 'extract_articles' main.py` -- should show at least 1.
  </verify>
  <done>
main.py wires the complete pipeline: source construction with API keys from env vars, per-source circuit breakers, rate-limit-aware schedulers, query executor with checkpoint/resume, article extraction, dedup+filter, and organized JSON/CSV output with metadata. Checkpoint is loaded on start and deleted on successful completion. Sources are closed in finally block.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.query._executor import QueryExecutor"` succeeds
2. `python -c "import main"` succeeds (import only, does not run pipeline)
3. `grep 'checkpoint' src/query/_executor.py main.py` shows integration in both files
4. `grep 'CircuitBreaker' main.py` shows per-source circuit breaker construction
5. `grep 'write_collection_output' main.py` shows output stage wired
6. `grep 'deduplicate_and_filter' main.py` shows dedup stage wired
7. `grep 'extract_articles' main.py` shows extraction stage wired
8. `grep 'run_collection' main.py` shows query collection stage wired
9. The pipeline stages run in order: collection -> extraction -> dedup -> output
</verification>

<success_criteria>
- QueryExecutor._execute_query_list() skips queries found in checkpoint
- QueryExecutor saves checkpoint after each individual query completion
- main.py constructs full pipeline: 3 sources, 3 circuit breakers, 3 schedulers, 1 executor, 1 checkpoint
- main.py runs stages in order: collection -> extraction -> dedup+filter -> output
- main.py builds CollectionMetadata with counts and sources
- main.py calls write_collection_output with filtered articles
- Checkpoint loaded on start, deleted on successful completion
- Sources closed in finally block regardless of outcome
- API keys read from environment variables (graceful degradation if missing)
</success_criteria>

<output>
After completion, create `.planning/phases/09-output-and-reliability/09-03-SUMMARY.md`
</output>
