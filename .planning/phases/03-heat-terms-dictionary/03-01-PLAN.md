---
phase: 03-heat-terms-dictionary
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/heat_terms.json
  - src/data/heat_terms_loader.py
autonomous: true

must_haves:
  truths:
    - "English terms cover all 8 categories with formal, journalistic, and colloquial registers"
    - "Hindi terms cover all 8 categories with formal, colloquial, journalistic, and borrowed registers"
    - "Borrowed English terms (heat wave, heat stroke, load shedding, red alert, orange alert, yellow alert) appear in Hindi term set in Devanagari script"
    - "Terms are structured data loadable by Python code, not free text"
    - "Loader validates the JSON at load time using Pydantic and caches the result"
  artifacts:
    - path: "src/data/heat_terms.json"
      provides: "Structured heat terms dictionary with en and hi languages"
      contains: "heatwave"
    - path: "src/data/heat_terms_loader.py"
      provides: "Pydantic-validated loader with cached loading and query functions"
      exports: ["load_heat_terms", "get_terms_for_language", "get_terms_by_category", "get_borrowed_terms", "get_all_term_languages"]
  key_links:
    - from: "src/data/heat_terms_loader.py"
      to: "src/data/heat_terms.json"
      via: "json.loads with Path(__file__).parent resolution"
      pattern: "_DATA_DIR.*heat_terms\\.json"
    - from: "src/data/heat_terms_loader.py"
      to: "pydantic BaseModel"
      via: "HeatTermsDictionary.model_validate"
      pattern: "model_validate"
---

<objective>
Create the heat terms dictionary JSON structure and Pydantic-validated loader with English and Hindi term sets.

Purpose: Establish the structured data format and loader code that all 14 languages will use, populated initially with the two most term-rich languages (English ~55 terms, Hindi ~70 terms) to validate the schema works correctly.

Output: `src/data/heat_terms.json` with en/hi terms, `src/data/heat_terms_loader.py` with loader + query API.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-heat-terms-dictionary/03-RESEARCH.md
@.planning/research/HEAT_TERMS_RESEARCH.md
@src/data/geo_loader.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create heat_terms.json with schema structure and English/Hindi terms</name>
  <files>src/data/heat_terms.json</files>
  <action>
Create `src/data/heat_terms.json` with the following structure. Populate English and Hindi terms from `.planning/research/HEAT_TERMS_RESEARCH.md`.

**JSON schema:**
```json
{
  "version": "1.0.0",
  "languages": {
    "<lang_code>": {
      "name": "<Language Name>",
      "categories": {
        "<category_key>": {
          "terms": [
            { "term": "<native script term>", "register": "<formal|colloquial|journalistic|borrowed>" }
          ]
        }
      }
    }
  }
}
```

**8 category keys (snake_case):** `heatwave`, `death_stroke`, `water_crisis`, `power_cuts`, `crop_damage`, `human_impact`, `government_response`, `temperature`

**4 register values:** `formal`, `colloquial`, `journalistic`, `borrowed`

**For English (en):** Extract ALL terms from the HEAT_TERMS_RESEARCH.md English section (Section 1). Map the research register labels to the 4 allowed values:
- "formal/IMD", "formal", "medical/formal", "formal/govt", "IMD classification" -> `formal`
- "colloquial/medical", "colloquial", "cultural/borrowed" -> `colloquial`
- "journalistic", "descriptive", "literary/journalistic" -> `journalistic`
- "borrowed English" -> `borrowed` (not applicable for English itself)
- "general", "medical", "technical" -> `formal`
- "South Asian English" -> `colloquial`

Map research category names to JSON keys:
- "heatwave" -> `heatwave`
- "death/stroke" -> `death_stroke`
- "water" -> `water_crisis`
- "power" -> `power_cuts`
- "crop" -> `crop_damage`
- "human" -> `human_impact`
- "govt" -> `government_response`
- "temperature" -> `temperature`

Include ALL terms from the research doc including MEDIUM and LOW confidence terms (high recall principle).

**For Hindi (hi):** Extract ALL terms from Section 2 of HEAT_TERMS_RESEARCH.md. Same register/category mapping. Hindi terms MUST be in Devanagari script (not romanized). Include ALL borrowed English terms in Devanagari (हीट वेव, हीट स्ट्रोक, लोड शेडिंग, रेड अलर्ट, ऑरेंज अलर्ट, येलो अलर्ट, एडवाइजरी, हीट एक्शन प्लान, डिहाइड्रेशन, सन स्ट्रोक).

**Critical details:**
- File encoding: UTF-8
- Native script directly in JSON (NOT Unicode escape sequences)
- Each language MUST have all 8 categories
- Each category MUST have at least 1 term
- Duplicate terms across categories are allowed and intentional (e.g., "लू" in both heatwave and death_stroke contexts)
- The file will be extended with 12 more languages in plan 03-02
  </action>
  <verify>
Run: `python -c "import json; d=json.load(open('src/data/heat_terms.json', encoding='utf-8')); assert set(d['languages'].keys()) == {'en', 'hi'}; [assert set(d['languages'][l]['categories'].keys()) == {'heatwave','death_stroke','water_crisis','power_cuts','crop_damage','human_impact','government_response','temperature'} for l in d['languages']]; print(f'en: {sum(len(c[\"terms\"]) for c in d[\"languages\"][\"en\"][\"categories\"].values())} terms'); print(f'hi: {sum(len(c[\"terms\"]) for c in d[\"languages\"][\"hi\"][\"categories\"].values())} terms')"` -- both languages have all 8 categories, en has 35+ terms, hi has 55+ terms.
  </verify>
  <done>heat_terms.json exists with version field, en and hi languages each having all 8 categories populated with native-script terms from the research document. Hindi borrowed terms are in Devanagari script.</done>
</task>

<task type="auto">
  <name>Task 2: Create heat_terms_loader.py with Pydantic models and query API</name>
  <files>src/data/heat_terms_loader.py</files>
  <action>
Create `src/data/heat_terms_loader.py` following the exact same architecture as `src/data/geo_loader.py`.

**Pydantic models (all frozen):**

1. `HeatTerm` -- single term entry:
   - `term: str` with `Field(..., min_length=1)`
   - `register: Literal["formal", "colloquial", "journalistic", "borrowed"]`

2. `CategoryTerms` -- terms within a category:
   - `terms: list[HeatTerm]` with `Field(min_length=1)`

3. `LanguageTerms` -- all categories for a language:
   - `name: str`
   - `categories: dict[TermCategory, CategoryTerms]`
   - `field_validator("categories")` that checks all 8 categories are present, raises ValueError listing missing ones

4. `HeatTermsDictionary` -- top-level container:
   - `version: str`
   - `languages: dict[str, LanguageTerms]`

**Type aliases:**
- `TermCategory = Literal["heatwave", "death_stroke", "water_crisis", "power_cuts", "crop_damage", "human_impact", "government_response", "temperature"]`
- `TermRegister = Literal["formal", "colloquial", "journalistic", "borrowed"]`
- `TERM_CATEGORIES: frozenset[str]` containing all 8 category names

**Cached loader:**
- `load_heat_terms() -> HeatTermsDictionary` with `@lru_cache(maxsize=1)`
- Uses `_DATA_DIR / "heat_terms.json"` with `encoding="utf-8"`
- Exactly mirrors `load_geo_data()` pattern

**Query functions (all delegate to `load_heat_terms()`):**

1. `get_terms_for_language(lang: str) -> list[str]` -- all terms flattened across all categories for a language. Returns empty list if language not in dictionary.

2. `get_terms_by_category(lang: str, category: TermCategory) -> list[str]` -- terms for a specific language+category. Returns empty list if language or category not found.

3. `get_borrowed_terms(lang: str) -> list[str]` -- only terms with register=="borrowed" for a language. Returns empty list if language not found.

4. `get_all_term_languages() -> list[str]` -- all language codes in the dictionary.

5. `get_terms_by_register(lang: str, register: TermRegister) -> list[str]` -- all terms with a specific register for a language. Returns empty list if language not found.

**Module docstring:** Follow the same style as geo_loader.py.

**Important:** Do NOT import from geo_loader.py. The heat_terms_loader is independent. Both use the same patterns but are separate modules.
  </action>
  <verify>
Run: `python -c "from src.data.heat_terms_loader import load_heat_terms, get_terms_for_language, get_terms_by_category, get_borrowed_terms, get_all_term_languages, get_terms_by_register, TERM_CATEGORIES; d = load_heat_terms(); print(f'Version: {d.version}'); print(f'Languages: {get_all_term_languages()}'); en = get_terms_for_language('en'); print(f'EN total: {len(en)}'); hi_borrowed = get_borrowed_terms('hi'); print(f'HI borrowed: {len(hi_borrowed)} -> {hi_borrowed[:3]}'); assert len(TERM_CATEGORIES) == 8; print('All checks passed')"` -- loads without error, returns correct counts, borrowed terms are in Devanagari.
  </verify>
  <done>heat_terms_loader.py provides Pydantic-validated loading with lru_cache, all 5 query functions work correctly, returns empty lists for unknown languages (no crashes), and validates all 8 categories are present for every language at load time.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.data.heat_terms_loader import load_heat_terms; d = load_heat_terms(); print('Loaded:', d.version)"` -- loads without Pydantic validation errors
2. `python -c "from src.data.heat_terms_loader import get_terms_for_language; t = get_terms_for_language('hi'); assert any('लू' in term for term in t), 'Missing loo term'; print('Hindi loo check: PASS')"` -- iconic Hindi heat term present
3. `python -c "from src.data.heat_terms_loader import get_borrowed_terms; b = get_borrowed_terms('hi'); assert any('हीट वेव' in t for t in b), 'Missing borrowed heat wave'; print('Borrowed terms check: PASS')"` -- borrowed terms in Devanagari
4. `python -c "from src.data.heat_terms_loader import get_terms_for_language; assert get_terms_for_language('zz') == [], 'Should return empty for unknown lang'; print('Unknown lang check: PASS')"` -- graceful handling
</verification>

<success_criteria>
- heat_terms.json is valid UTF-8 JSON with en and hi language entries
- Each language has all 8 categories populated with 1+ terms each
- Hindi terms are in Devanagari script, not romanized
- Borrowed English terms appear in Hindi in Devanagari (e.g., हीट वेव)
- heat_terms_loader.py validates the JSON at load time with Pydantic
- All 5 query functions return correct results
- Pattern matches geo_loader.py exactly (frozen models, lru_cache, Path(__file__).parent)
</success_criteria>

<output>
After completion, create `.planning/phases/03-heat-terms-dictionary/03-01-SUMMARY.md`
</output>
