---
phase: 03-heat-terms-dictionary
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/data/heat_terms.json
  - src/data/__init__.py
autonomous: true

must_haves:
  truths:
    - "All 14 languages (en, hi, ta, te, bn, mr, gu, kn, ml, or, pa, as, ur, ne) have terms in the dictionary"
    - "Each of the 14 languages covers all 8 term categories"
    - "Tamil terms include culturally unique terms like அக்னி நட்சத்திரம் (agni nakshatram)"
    - "Bengali terms include দাবদাহ (dabdaho), Telugu includes వడ గాలులు (vada gaalulu)"
    - "Borrowed English terms (heat wave, heat stroke, load shedding, red alert) are transliterated in each language's native script"
    - "Heat terms functions are importable from src.data package"
  artifacts:
    - path: "src/data/heat_terms.json"
      provides: "Complete 14-language heat terms dictionary"
      contains: "அக்னி நட்சத்திரம்"
    - path: "src/data/__init__.py"
      provides: "Re-exports for heat terms loader functions"
      contains: "get_terms_for_language"
  key_links:
    - from: "src/data/__init__.py"
      to: "src/data/heat_terms_loader.py"
      via: "from .heat_terms_loader import"
      pattern: "from \\.heat_terms_loader import"
    - from: "src/data/heat_terms_loader.py"
      to: "src/data/heat_terms.json"
      via: "Pydantic validation of all 14 languages"
      pattern: "model_validate"
---

<objective>
Populate the remaining 12 languages (ta, te, bn, mr, gu, kn, ml, or, pa, as, ur, ne) into the heat terms dictionary and wire up the __init__.py re-exports.

Purpose: Complete the multilingual heat terms dictionary so it covers all 14 Indian languages across all 8 categories, ready for Phase 6 query generation.

Output: Updated `src/data/heat_terms.json` with all 14 languages, updated `src/data/__init__.py` re-exporting heat terms functions.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-heat-terms-dictionary/03-RESEARCH.md
@.planning/research/HEAT_TERMS_RESEARCH.md
@.planning/phases/03-heat-terms-dictionary/03-01-SUMMARY.md
@src/data/heat_terms_loader.py
@src/data/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add remaining 12 language term sets to heat_terms.json</name>
  <files>src/data/heat_terms.json</files>
  <action>
Read the existing `src/data/heat_terms.json` (contains en, hi from plan 03-01). Add 12 more language entries by extracting terms from `.planning/research/HEAT_TERMS_RESEARCH.md` sections 3-14.

**Languages to add (in this order):**
- Tamil (ta) -- Section 3
- Telugu (te) -- Section 4
- Bengali (bn) -- Section 5
- Marathi (mr) -- Section 6
- Gujarati (gu) -- Section 7
- Kannada (kn) -- Section 8
- Malayalam (ml) -- Section 9
- Odia (or) -- Section 10
- Punjabi (pa) -- Section 11
- Assamese (as) -- Section 12
- Urdu (ur) -- Section 13
- Nepali (ne) -- Section 14

**For each language, extract ALL terms from the research doc into the correct categories and registers:**

Register mapping (research labels -> JSON values):
- "formal/IMD Tamil", "formal", "formal/medical", "formal/journalistic" -> `formal`
- "colloquial", "cultural/colloquial", "colloquial, very common" -> `colloquial`
- "journalistic", "descriptive" -> `journalistic`
- "borrowed English" -> `borrowed`

Category mapping (same as plan 03-01):
- heatwave terms -> `heatwave`
- death/stroke terms -> `death_stroke`
- water terms -> `water_crisis`
- power terms -> `power_cuts`
- crop terms -> `crop_damage`
- human terms -> `human_impact`
- govt terms -> `government_response`
- temperature terms -> `temperature`

**Critical requirements:**
1. ALL terms in native script (Tamil script for ta, Telugu script for te, Bengali script for bn, etc.). NEVER romanized.
2. Include ALL confidence levels (HIGH, MEDIUM, LOW) -- high recall principle.
3. Each language MUST have all 8 categories. If a category has sparse native terms, it MUST at minimum have the borrowed English terms transliterated in that language's script.
4. Borrowed English terms that MUST appear in every language (in that language's script):
   - "heat wave" / "heatwave" -> in heatwave category
   - "heat stroke" -> in death_stroke category
   - "load shedding" -> in power_cuts category
   - "red alert", "orange alert", "yellow alert" -> in government_response category

   If the research doc has these transliterations, use them. If not for a specific language, infer the transliteration following the script conventions observed in other terms for that language.

5. Preserve culturally unique terms that must not be lost:
   - Tamil: அக்னி நட்சத்திரம் (agni nakshatram) in heatwave
   - Bengali: দাবদাহ (dabdaho) in heatwave
   - Telugu: వడ గాలులు (vada gaalulu) in heatwave
   - Marathi: भारनियमन (bhaarniyaman) in power_cuts

6. Urdu terms MUST be in Nastaliq/Arabic script (right-to-left), NOT Devanagari.

7. File must remain valid UTF-8 JSON after all additions.

8. After adding all languages, verify the total is 14 language entries.
  </action>
  <verify>
Run: `python -c "
import json
d = json.load(open('src/data/heat_terms.json', encoding='utf-8'))
langs = sorted(d['languages'].keys())
expected = sorted(['en','hi','ta','te','bn','mr','gu','kn','ml','or','pa','as','ur','ne'])
assert langs == expected, f'Missing: {set(expected)-set(langs)}'
cats = {'heatwave','death_stroke','water_crisis','power_cuts','crop_damage','human_impact','government_response','temperature'}
for lang in langs:
    lang_cats = set(d['languages'][lang]['categories'].keys())
    assert lang_cats == cats, f'{lang} missing: {cats - lang_cats}'
    for cat in cats:
        assert len(d['languages'][lang]['categories'][cat]['terms']) >= 1, f'{lang}/{cat} empty'
total = sum(sum(len(c['terms']) for c in l['categories'].values()) for l in d['languages'].values())
print(f'Total: {len(langs)} languages, {total} terms')
print('All 14 languages have all 8 categories: PASS')
"` -- all 14 languages present, all 8 categories per language, 450+ total terms.
  </verify>
  <done>heat_terms.json contains all 14 languages, each with all 8 categories populated with native-script terms. Culturally unique terms preserved. Borrowed English terms present in every regional language in that language's script. Total term count is 450+.</done>
</task>

<task type="auto">
  <name>Task 2: Update src/data/__init__.py to re-export heat terms functions</name>
  <files>src/data/__init__.py</files>
  <action>
Update `src/data/__init__.py` to re-export heat terms loader functions, following the exact same pattern as the existing geo_loader re-exports.

**Add these imports:**
```python
from .heat_terms_loader import (
    CategoryTerms,
    HeatTerm,
    HeatTermsDictionary,
    LanguageTerms,
    TERM_CATEGORIES,
    get_all_term_languages,
    get_borrowed_terms,
    get_terms_by_category,
    get_terms_by_register,
    get_terms_for_language,
    load_heat_terms,
)
```

**Update the `__all__` list** to include all new symbols (append after existing geo entries).

**Update the module docstring** to mention heat terms alongside geographic data:
```python
"""Geographic data and heat terms dictionary for the heat news extraction pipeline."""
```

Keep existing geo_loader imports unchanged.
  </action>
  <verify>
Run: `python -c "from src.data import load_heat_terms, get_terms_for_language, get_terms_by_category, get_borrowed_terms, get_all_term_languages, get_terms_by_register, TERM_CATEGORIES, HeatTerm, CategoryTerms, LanguageTerms, HeatTermsDictionary; print('All heat terms symbols importable from src.data'); d = load_heat_terms(); print(f'Loaded {len(d.languages)} languages via src.data import')"` -- all symbols importable via the package.
  </verify>
  <done>src/data/__init__.py re-exports all heat terms loader models and functions. Importing from src.data gives access to both geo data and heat terms without needing to know the internal module structure.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.data.heat_terms_loader import load_heat_terms; d = load_heat_terms(); assert len(d.languages) == 14; print(f'Loaded {len(d.languages)} languages')"` -- Pydantic validates all 14 languages
2. `python -c "from src.data import get_terms_for_language; ta = get_terms_for_language('ta'); assert any('அக்னி நட்சத்திரம்' in t for t in ta); print('Tamil agni nakshatram: PASS')"` -- culturally unique Tamil term present
3. `python -c "from src.data import get_terms_for_language; bn = get_terms_for_language('bn'); assert any('দাবদাহ' in t for t in bn); print('Bengali dabdaho: PASS')"` -- culturally unique Bengali term present
4. `python -c "from src.data import get_borrowed_terms; langs = ['ta','te','bn','mr','gu','kn','ml','or','pa','as','ur','ne']; [print(f'{l}: {len(get_borrowed_terms(l))} borrowed') for l in langs]"` -- every regional language has borrowed terms
5. `python -c "from src.data import get_all_term_languages; assert sorted(get_all_term_languages()) == sorted(['en','hi','ta','te','bn','mr','gu','kn','ml','or','pa','as','ur','ne']); print('All 14 languages: PASS')"` -- complete language coverage
6. `python -c "from src.data import load_geo_data, load_heat_terms; g = load_geo_data(); h = load_heat_terms(); print(f'Geo: {len(g.states)} regions, Heat: {len(h.languages)} languages -- both work')"` -- geo loader still works after __init__.py changes
</verification>

<success_criteria>
- heat_terms.json contains exactly 14 language entries (en, hi, ta, te, bn, mr, gu, kn, ml, or, pa, as, ur, ne)
- Each language has all 8 categories with 1+ terms each
- All terms are in their language's native script
- Borrowed English terms transliterated in every regional language's script
- Culturally unique terms preserved (agni nakshatram, dabdaho, vada gaalulu, bhaarniyaman)
- Total term count is 450+
- src/data/__init__.py re-exports all heat terms symbols
- Existing geo_loader imports still work after __init__.py update
- `from src.data import get_terms_for_language` works
</success_criteria>

<output>
After completion, create `.planning/phases/03-heat-terms-dictionary/03-02-SUMMARY.md`
</output>
