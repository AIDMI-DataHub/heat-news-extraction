---
phase: 08-deduplication-and-filtering
plan: 02
type: tdd
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/data/exclusion_patterns.json
  - src/dedup/_relevance.py
  - src/dedup/__init__.py
  - tests/test_relevance.py
autonomous: true

must_haves:
  truths:
    - "Each article receives a relevance_score (0.0-1.0) based on heat term presence in title + full_text and category diversity"
    - "Articles with full_text=None still score based on title alone (baseline 0.3+ if heat terms found in title)"
    - "A configurable exclusion_patterns.json exists and can be updated without code changes to add new irrelevant patterns"
    - "Exclusion patterns are conjunctive (cricket + score, not cricket alone) to avoid removing legitimate heat articles mentioning sports venues"
    - "Only articles scoring below 0.05 AND matching an exclusion pattern are filtered out (high recall -- borderline kept)"
    - "The deduplicate_and_filter() pipeline function composes URL dedup -> title dedup -> score -> filter in correct order"
  artifacts:
    - path: "src/data/exclusion_patterns.json"
      provides: "Configurable irrelevant pattern list"
      contains: "patterns"
    - path: "src/dedup/_relevance.py"
      provides: "score_relevance() and filter_articles()"
      exports: ["score_relevance", "filter_articles"]
    - path: "src/dedup/__init__.py"
      provides: "Public API: deduplicate_and_filter, deduplicate_by_url, deduplicate_by_title, score_relevance, filter_articles"
      exports: ["deduplicate_and_filter"]
    - path: "tests/test_relevance.py"
      provides: "Unit tests for relevance scoring and filtering"
      min_lines: 60
  key_links:
    - from: "src/dedup/_relevance.py"
      to: "src/data/heat_terms_loader.py"
      via: "imports get_terms_by_category and TERM_CATEGORIES"
      pattern: "from src\\.data\\.heat_terms_loader import"
    - from: "src/dedup/_relevance.py"
      to: "src/data/exclusion_patterns.json"
      via: "loads exclusion patterns from JSON file"
      pattern: "exclusion_patterns\\.json"
    - from: "src/dedup/_relevance.py"
      to: "src/models/article.py"
      via: "imports Article, uses model_copy(update=) for frozen model"
      pattern: "model_copy.*relevance_score"
    - from: "src/dedup/__init__.py"
      to: "src/dedup/_url_dedup.py"
      via: "imports deduplicate_by_url"
      pattern: "from src\\.dedup\\._url_dedup import"
    - from: "src/dedup/__init__.py"
      to: "src/dedup/_title_dedup.py"
      via: "imports deduplicate_by_title"
      pattern: "from src\\.dedup\\._title_dedup import"
    - from: "src/dedup/__init__.py"
      to: "src/dedup/_relevance.py"
      via: "imports score_relevance, filter_articles"
      pattern: "from src\\.dedup\\._relevance import"
---

<objective>
Implement relevance scoring, configurable exclusion patterns, and the complete dedup+filter pipeline composition.

Purpose: After deduplication, each article needs a relevance score (0.0-1.0) based on heat term presence and category diversity. Clearly irrelevant content (cricket scores, horoscopes, summer fashion) is excluded via configurable patterns. The pipeline's core value is high recall -- borderline articles are kept. This plan also assembles the full `deduplicate_and_filter()` pipeline function that composes URL dedup, title dedup, scoring, and filtering.

Output: `exclusion_patterns.json` data file, `_relevance.py` module with scoring and filtering, `__init__.py` with pipeline composition and re-exports, plus comprehensive tests.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/models/article.py
@src/data/heat_terms_loader.py
@.planning/phases/08-deduplication-and-filtering/08-RESEARCH.md
@.planning/phases/08-deduplication-and-filtering/08-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Exclusion patterns data file and relevance scoring</name>
  <files>src/data/exclusion_patterns.json, src/dedup/_relevance.py, tests/test_relevance.py</files>
  <action>
**Create `src/data/exclusion_patterns.json`:**
```json
{
  "version": "1.0.0",
  "description": "Patterns indicating clearly irrelevant content. Update this file without code changes to add new patterns. Each pattern is a regex checked against article title + text. Patterns are intentionally conjunctive (e.g., cricket AND score) to avoid excluding legitimate heat articles mentioning sports venues.",
  "patterns": [
    {"pattern": "\\bcricket\\b.*\\b(score|scorecard|innings|wicket|runs)\\b", "category": "sports", "description": "Cricket match scores and scorecards"},
    {"pattern": "\\bIPL\\b.*\\b(match|final|semi|qualifier|auction)\\b", "category": "sports", "description": "IPL cricket events"},
    {"pattern": "\\b(football|soccer|tennis|badminton)\\b.*\\b(score|match|final|tournament)\\b", "category": "sports", "description": "Other sports scores"},
    {"pattern": "\\bweather\\s+forecast\\b.*\\b(tomorrow|next\\s+week|weekend|today|monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b", "category": "weather_forecast", "description": "Generic day-specific weather forecasts"},
    {"pattern": "\\bsummer\\s+(fashion|style|recipe|vacation|getaway|travel|destination|outfit)\\b", "category": "lifestyle", "description": "Summer lifestyle content"},
    {"pattern": "\\bhoroscope\\b", "category": "astrology", "description": "Horoscope/astrology content"},
    {"pattern": "\\b(hot|heat)\\s+(deal|sale|offer|discount|price)\\b", "category": "marketing", "description": "Marketing using heat terminology"},
    {"pattern": "\\b(hot|heat|sizzling)\\s+(new|latest|upcoming)\\s+(movie|film|song|album|trailer|release)\\b", "category": "entertainment", "description": "Entertainment using heat metaphors"},
    {"pattern": "\\bhot\\s+(stock|share|investment|crypto|bitcoin)\\b", "category": "finance", "description": "Financial content using heat metaphors"}
  ]
}
```

**Create `src/dedup/_relevance.py`** with these functions:

1. `_load_exclusion_patterns() -> list[re.Pattern[str]]` -- Private, cached with `@lru_cache(maxsize=1)`:
   - Load from `Path(__file__).resolve().parent.parent / "data" / "exclusion_patterns.json"`
   - Parse JSON, compile each pattern entry's "pattern" field with `re.IGNORECASE`
   - Return list of compiled regex patterns

2. `_combine_text(article: Article) -> str` -- Private helper:
   - Combine `article.title` and `article.full_text` (if not None) into a single lowercase string
   - Separator: newline
   - If both are empty/None, return empty string

3. `_matches_exclusion(text: str, patterns: list[re.Pattern[str]]) -> bool` -- Private:
   - Return `True` if ANY pattern in the list matches against the text
   - Uses `pattern.search(text)` (not `.match()` -- search finds anywhere in string)

4. `score_relevance(article: Article) -> float` -- Public:
   - Get combined text via `_combine_text(article)`
   - If no text at all (empty string), return 0.0
   - Import `get_terms_by_category` and `TERM_CATEGORIES` from `src.data.heat_terms_loader`
   - For each category, check which terms appear in the combined text (case-insensitive `term.lower() in text`)
   - Track `matched_terms: set[str]` and `matched_categories: set[str]`
   - **Scoring formula:**
     - `term_score = min(len(matched_terms) / 3.0, 1.0)` -- 3+ terms = full score
     - `category_score = min(len(matched_categories) / 2.0, 1.0)` -- 2+ categories = full score
     - `title_terms = count of matched_terms that appear in article.title.lower()`
     - `title_bonus = 0.2 if title_terms > 0 else 0.0`
     - `raw_score = (term_score * 0.5) + (category_score * 0.3) + title_bonus`
     - Return `min(raw_score, 1.0)`
   - **Special case for full_text=None:** If `article.full_text is None` but title has matched terms, use a floor of 0.3 (title-only articles should not be penalized for extraction failure)

5. `filter_articles(articles: list[Article]) -> list[Article]` -- Public:
   - Score each article: `scored = article.model_copy(update={"relevance_score": score_relevance(article)})`
   - Load exclusion patterns via `_load_exclusion_patterns()`
   - **High-recall filter logic:** Keep article if:
     - `relevance_score >= 0.05`, OR
     - Does NOT match any exclusion pattern (even if score is low, keep it unless it matches an exclusion pattern)
   - In other words: exclude ONLY if `relevance_score < 0.05 AND matches_exclusion is True`
   - Log: "Relevance filter: {before} -> {after} articles (excluded {diff})"
   - Return the list of scored+filtered articles (with updated relevance_score)

**TDD: Write tests FIRST in `tests/test_relevance.py`:**

RED tests for `score_relevance`:
- `test_scores_article_with_multiple_heat_terms` -- Article mentioning "heatwave", "heat stroke", "temperature" scores > 0.5
- `test_scores_zero_for_no_heat_terms` -- Article about "cricket match in Mumbai" with no heat terms scores 0.0
- `test_title_bonus_increases_score` -- Article with heat terms in title scores higher than same terms only in full_text
- `test_full_text_none_with_heat_title_scores_above_zero` -- Article with `full_text=None` but "heatwave" in title scores >= 0.3
- `test_category_diversity_bonus` -- Article matching 2+ categories scores higher than article matching only 1

RED tests for `filter_articles`:
- `test_keeps_high_score_article` -- Article with heatwave content is kept
- `test_excludes_cricket_score_article` -- Article about "India cricket score 350 runs" with no heat terms is excluded
- `test_keeps_borderline_article` -- Article with low score (0.1) but no exclusion pattern match is KEPT (high recall)
- `test_keeps_article_with_heat_and_cricket` -- Article mentioning both heatwave and cricket (e.g., "cricket match suspended due to heatwave") is KEPT
- `test_updates_relevance_score` -- After filtering, each article has its `relevance_score` updated from 0.0 to actual score

For test Article construction: use `Article(title=..., url="https://example.com/1", source="Test", date=datetime(2024, 6, 1, tzinfo=ZoneInfo("Asia/Kolkata")), language="en", state="Rajasthan", search_term="heatwave", full_text=..., relevance_score=0.0)`.

Then GREEN: implement to pass all tests.
  </action>
  <verify>Run `python -m pytest tests/test_relevance.py -v` -- all relevance scoring and filtering tests pass.</verify>
  <done>score_relevance produces 0.0-1.0 based on heat term presence + category diversity + title bonus. filter_articles excludes only articles with score < 0.05 AND matching exclusion patterns. full_text=None articles with heat titles score >= 0.3. All tests green.</done>
</task>

<task type="auto">
  <name>Task 2: Pipeline composition and package re-exports</name>
  <files>src/dedup/__init__.py</files>
  <action>
**Update `src/dedup/__init__.py`** (created as empty in Plan 01) with:

1. Module docstring explaining the package purpose.

2. Import and re-export public API:
   - `from src.dedup._url_dedup import normalize_url, deduplicate_by_url`
   - `from src.dedup._title_dedup import deduplicate_by_title`
   - `from src.dedup._relevance import score_relevance, filter_articles`

3. **Create `deduplicate_and_filter(articles: list[Article]) -> list[Article]`** -- the main pipeline entry point:
   - Stage 1: `deduped_url = deduplicate_by_url(articles)` -- URL dedup
   - Stage 2: `deduped_title = deduplicate_by_title(deduped_url, threshold=0.85)` -- Title dedup
   - Stage 3: `filtered = filter_articles(deduped_title)` -- Scores + filters
   - Log total pipeline summary: "Dedup+filter pipeline: {input} -> {output} articles"
   - Return `filtered`

4. `__all__` list: `["deduplicate_and_filter", "deduplicate_by_url", "deduplicate_by_title", "normalize_url", "score_relevance", "filter_articles"]`

This function is the public API consumed by downstream phases (Phase 9 output). It takes a raw `list[Article]` from extraction and returns a deduplicated, scored, and filtered `list[Article]`.
  </action>
  <verify>
Run `python -c "from src.dedup import deduplicate_and_filter; print('Pipeline OK')"` -- import succeeds.
Run `python -m pytest tests/test_dedup.py tests/test_relevance.py -v` -- all tests still pass.
  </verify>
  <done>deduplicate_and_filter() composes URL dedup -> title dedup -> scoring -> filtering in correct order. Package __init__.py re-exports all public symbols. All tests pass.</done>
</task>

</tasks>

<verification>
```bash
# All tests pass
python -m pytest tests/test_dedup.py tests/test_relevance.py -v

# Pipeline function importable and callable
python -c "from src.dedup import deduplicate_and_filter, deduplicate_by_url, deduplicate_by_title, score_relevance, filter_articles; print('All exports OK')"

# Exclusion patterns file is valid JSON
python -c "import json; json.loads(open('src/data/exclusion_patterns.json').read()); print('JSON valid')"

# No new dependencies
pip freeze | wc -l  # Same count as before
```
</verification>

<success_criteria>
- exclusion_patterns.json contains conjunctive patterns for sports, weather forecasts, lifestyle, marketing
- score_relevance uses term presence + category diversity + title bonus, returns 0.0-1.0
- Articles with full_text=None score >= 0.3 if title has heat terms (not penalized for extraction failure)
- filter_articles excludes ONLY articles with score < 0.05 AND matching exclusion pattern (high recall)
- deduplicate_and_filter composes the full pipeline: URL dedup -> title dedup -> score -> filter
- src/dedup/__init__.py exports all public symbols
- All tests pass
- Zero new dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/08-deduplication-and-filtering/08-02-SUMMARY.md`
</output>
