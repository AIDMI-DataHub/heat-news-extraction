---
phase: 08-deduplication-and-filtering
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/dedup/__init__.py
  - src/dedup/_url_dedup.py
  - src/dedup/_title_dedup.py
  - tests/test_dedup.py
autonomous: true

must_haves:
  truths:
    - "Articles with identical normalized URLs (tracking params removed, www stripped, sorted query) are deduplicated -- only the higher-quality version is kept"
    - "Articles with highly similar titles (>= 0.85 SequenceMatcher ratio after source suffix stripping) within the same language are deduplicated"
    - "When duplicates are found, the version with longer full_text and more metadata wins"
    - "Title dedup only compares within same-language buckets, never cross-language"
    - "Articles with full_text=None are never discarded by dedup (they may be the only version)"
  artifacts:
    - path: "src/dedup/_url_dedup.py"
      provides: "normalize_url() and deduplicate_by_url()"
      exports: ["normalize_url", "deduplicate_by_url"]
    - path: "src/dedup/_title_dedup.py"
      provides: "title_similarity() and deduplicate_by_title()"
      exports: ["deduplicate_by_title"]
    - path: "tests/test_dedup.py"
      provides: "Unit tests for URL and title deduplication"
      min_lines: 80
  key_links:
    - from: "src/dedup/_url_dedup.py"
      to: "src/models/article.py"
      via: "imports Article model"
      pattern: "from src\\.models\\.article import Article"
    - from: "src/dedup/_title_dedup.py"
      to: "src/models/article.py"
      via: "imports Article model"
      pattern: "from src\\.models\\.article import Article"
---

<objective>
Implement URL-based and title-based article deduplication using stdlib only.

Purpose: Articles are collected from multiple queries and multiple sources (Google News, NewsData.io, GNews). The same article frequently appears in results for different search terms, and the same story is published by different outlets with similar titles. Deduplication removes these duplicates while keeping the highest-quality version.

Output: Two dedup modules (`_url_dedup.py`, `_title_dedup.py`) with pure functions `list[Article] -> list[Article]`, plus comprehensive tests.
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/models/article.py
@.planning/phases/08-deduplication-and-filtering/08-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: URL deduplication with tracking parameter normalization</name>
  <files>src/dedup/__init__.py, src/dedup/_url_dedup.py, tests/test_dedup.py</files>
  <action>
**Create `src/dedup/__init__.py`** as an empty placeholder (Plan 02 will populate re-exports).

**Create `src/dedup/_url_dedup.py`** with two public functions:

1. `normalize_url(url: str) -> str` -- Normalizes a URL for dedup comparison:
   - Parse with `urllib.parse.urlparse()`
   - Lowercase scheme and netloc
   - Strip `www.` prefix from netloc using `.removeprefix("www.")`
   - Strip trailing slash from path (keep "/" if path is empty)
   - Remove tracking params from query string. Use a module-level `_TRACKING_PARAMS: frozenset[str]` containing: `utm_source`, `utm_medium`, `utm_campaign`, `utm_term`, `utm_content`, `utm_id`, `fbclid`, `gclid`, `yclid`, `msclkid`, `_ga`, `_gl`, `ref`, `source`, `mkt_tok`, `mc_cid`, `mc_eid`, `hsCtaTracking`, `si`, `__cft__`, `__tn__`
   - Sort remaining query params for deterministic comparison
   - Remove fragment
   - Reconstruct with `urlunparse()`

2. `deduplicate_by_url(articles: list[Article]) -> list[Article]` -- Deduplicates by normalized URL:
   - Build a `dict[str, Article]` keyed by `normalize_url(article.url)`
   - When a URL collision occurs, keep the article with higher `_quality_score()`
   - Return `list(seen.values())`
   - Log: "URL dedup: {before} -> {after} articles"

3. Private `_quality_score(article: Article) -> int` helper:
   - `+100 + len(full_text)` if `full_text is not None` (longer text = better extraction)
   - `+10` if `district is not None` (has district-level geo info)
   - `+5` if `source != "Unknown"` (has identified source)
   - Returns the total integer score

**TDD: Write tests FIRST in `tests/test_dedup.py`:**

RED tests for `normalize_url`:
- `test_strips_utm_params` -- URL with utm_source, utm_medium -> params removed
- `test_strips_www_prefix` -- `www.example.com` -> `example.com`
- `test_lowercase_scheme_and_host` -- `HTTP://Example.COM/Path` -> `http://example.com/Path` (path case preserved)
- `test_strips_trailing_slash` -- `/path/` -> `/path`
- `test_strips_fragment` -- `#section` removed
- `test_sorts_remaining_params` -- `b=2&a=1` -> `a=1&b=2`
- `test_preserves_non_tracking_params` -- `id=123` kept, `utm_source=x` removed

RED tests for `deduplicate_by_url`:
- `test_removes_exact_url_duplicates` -- Two articles with same URL, keeps higher quality
- `test_removes_tracking_param_duplicates` -- Same URL with different utm params -> deduplicated
- `test_keeps_different_urls` -- Different URLs both kept
- `test_keeps_higher_quality_with_full_text` -- Article with full_text wins over None
- `test_keeps_higher_quality_with_longer_text` -- Longer full_text wins

Use Pydantic model construction for test Article instances. Every Article needs: title, url, source, date (use `datetime(2024, 6, 1, tzinfo=ZoneInfo("Asia/Kolkata"))`), language ("en"), state ("Rajasthan"), search_term ("heatwave").

Then GREEN: implement to pass all tests.
  </action>
  <verify>Run `python -m pytest tests/test_dedup.py -v` -- all URL dedup tests pass.</verify>
  <done>normalize_url strips tracking params, www, fragment, trailing slash, lowercases, sorts params. deduplicate_by_url removes duplicates keeping higher quality. All tests green.</done>
</task>

<task type="auto">
  <name>Task 2: Title similarity deduplication with language bucketing</name>
  <files>src/dedup/_title_dedup.py, tests/test_dedup.py</files>
  <action>
**Create `src/dedup/_title_dedup.py`** with two public functions:

1. `_strip_source_suffix(title: str) -> str` -- Private helper that strips " - Source Name" suffix:
   - Find the last occurrence of " - " in the title
   - If found AND the part after it is <= 40 chars (source names are short), strip it
   - This catches "Heatwave kills 10 - Times of India" -> "Heatwave kills 10"
   - If not found, return title unchanged

2. `_title_similarity(title_a: str, title_b: str) -> float` -- Private helper:
   - Strip source suffixes from both titles
   - Strip whitespace, lowercase both (for Latin scripts; Devanagari/Tamil have no case)
   - Return `SequenceMatcher(None, a, b).ratio()`

3. `deduplicate_by_title(articles: list[Article], threshold: float = 0.85) -> list[Article]` -- Deduplicates by title similarity:
   - **Bucket articles by language** first (only compare within same language)
   - For each language bucket, iterate articles:
     - For each new article, check similarity against all `kept` articles in that bucket
     - If similarity >= threshold, it is a duplicate: keep the one with higher `_quality_score()` (import from `_url_dedup`)
     - If no match found, add to `kept`
   - Combine all language bucket results into a single flat list
   - Log: "Title dedup: {before} -> {after} articles"

**Import `_quality_score` from `_url_dedup`** -- do NOT duplicate it.

**TDD: Add tests to `tests/test_dedup.py`:**

RED tests for `_strip_source_suffix` (import as private for testing):
- `test_strips_source_suffix` -- "Heatwave in Delhi - NDTV" -> "Heatwave in Delhi"
- `test_no_suffix_unchanged` -- "Heatwave in Delhi" unchanged
- `test_long_suffix_preserved` -- If suffix after " - " is > 40 chars, it is likely part of the title, not a source name

RED tests for `deduplicate_by_title`:
- `test_removes_similar_titles_same_source_suffix` -- "Heatwave kills 10 - Times of India" vs "Heatwave kills 10 - NDTV" -> deduplicated
- `test_keeps_different_titles` -- "Heatwave in Delhi" vs "Flood in Mumbai" -> both kept
- `test_language_bucketing` -- Hindi article and English article with different titles both kept (no cross-language comparison)
- `test_keeps_higher_quality_duplicate` -- Between two similar-titled articles, the one with longer full_text is kept
- `test_threshold_boundary` -- Two titles just below threshold (e.g., 0.84) are both kept

Then GREEN: implement to pass all tests.
  </action>
  <verify>Run `python -m pytest tests/test_dedup.py -v` -- all title dedup tests pass (both Task 1 and Task 2 tests).</verify>
  <done>deduplicate_by_title groups articles by language, compares titles within each group using SequenceMatcher with source suffix stripping, keeps higher-quality duplicates. All tests green.</done>
</task>

</tasks>

<verification>
```bash
# All dedup tests pass
python -m pytest tests/test_dedup.py -v

# Both modules importable
python -c "from src.dedup._url_dedup import normalize_url, deduplicate_by_url; print('URL dedup OK')"
python -c "from src.dedup._title_dedup import deduplicate_by_title; print('Title dedup OK')"

# No new dependencies added
pip freeze | wc -l  # Same count as before
```
</verification>

<success_criteria>
- normalize_url correctly strips tracking params, www prefix, trailing slash, fragment, lowercases, sorts query params
- deduplicate_by_url removes URL duplicates keeping higher-quality version
- deduplicate_by_title removes title duplicates within same-language buckets using SequenceMatcher
- _quality_score correctly ranks articles by full_text length and metadata completeness
- All tests pass with `python -m pytest tests/test_dedup.py -v`
- Zero new dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/08-deduplication-and-filtering/08-01-SUMMARY.md`
</output>
