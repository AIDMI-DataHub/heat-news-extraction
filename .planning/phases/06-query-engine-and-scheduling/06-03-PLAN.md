---
phase: 06-query-engine-and-scheduling
plan: 03
type: execute
wave: 3
depends_on: ["06-01", "06-02"]
files_modified:
  - src/query/_executor.py
  - src/query/__init__.py
autonomous: true

must_haves:
  truths:
    - "State-level queries execute first across all sources in parallel"
    - "Only states with actual article results get district-level drill-down queries"
    - "All three sources execute concurrently (not sequentially) via asyncio.TaskGroup"
    - "The executor returns a flat list of all collected ArticleRef objects"
    - "The executor logs collection progress: query counts, active states, total articles"
  artifacts:
    - path: "src/query/_executor.py"
      provides: "QueryExecutor with hierarchical state-then-district execution"
      contains: "class QueryExecutor"
    - path: "src/query/__init__.py"
      provides: "Updated re-exports including QueryExecutor"
      contains: "QueryExecutor"
  key_links:
    - from: "src/query/_executor.py"
      to: "src/query/_generator.py"
      via: "uses QueryGenerator to produce queries"
      pattern: "QueryGenerator"
    - from: "src/query/_executor.py"
      to: "src/query/_scheduler.py"
      via: "dispatches queries through SourceScheduler instances"
      pattern: "SourceScheduler"
    - from: "src/query/_executor.py"
      to: "src/data/geo_loader.py"
      via: "loads regions via get_all_regions"
      pattern: "get_all_regions"
---

<objective>
Create the QueryExecutor that orchestrates hierarchical query execution: state-level queries first across all sources in parallel, then district-level queries only for states that returned results.

Purpose: This is the orchestration heart of the pipeline. It ties together the QueryGenerator (what to search) with the SourceSchedulers (how to search safely) into a two-phase hierarchical execution flow that maximizes coverage while respecting API limits. State queries guarantee baseline coverage; district queries provide depth only where heat news is active.

Output: `src/query/_executor.py` (QueryExecutor), updated `src/query/__init__.py`
</objective>

<execution_context>
@/Users/akashyadav/.claude/get-shit-done/workflows/execute-plan.md
@/Users/akashyadav/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-query-engine-and-scheduling/06-RESEARCH.md
@.planning/phases/06-query-engine-and-scheduling/06-01-SUMMARY.md
@.planning/phases/06-query-engine-and-scheduling/06-02-SUMMARY.md

@src/query/_models.py
@src/query/_generator.py
@src/query/_scheduler.py
@src/data/geo_loader.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QueryExecutor with hierarchical execution</name>
  <files>src/query/_executor.py</files>
  <action>
Create `src/query/_executor.py` with:

**class QueryExecutor:**

Constructor:
```python
def __init__(
    self,
    schedulers: dict[str, SourceScheduler],  # keyed by source name: "google", "newsdata", "gnews"
    generator: QueryGenerator,
):
```

Stores schedulers dict and generator. Creates logger.

Methods:

1. `async def run_collection(self, regions: list[StateUT] | None = None) -> list[ArticleRef]`:
   Main entry point. If regions is None, load all via `get_all_regions()`.

   **Phase 1 -- State-level queries:**
   - Call `self._generator.generate_state_queries(regions)` to get dict of queries keyed by source_hint.
   - Log total query counts per source.
   - Call `await self._execute_queries_parallel(queries_by_source)` to execute all across sources.
   - Collect all articles from results.

   **Determine active states:**
   - Build `active_slugs: set[str]` from results where `result.articles` is non-empty.
   - Log: `"{N} / {M} states have active heat news"`.

   **Phase 2 -- District-level queries (active states only):**
   - Filter regions to active_slugs only: `active_regions = [r for r in regions if r.slug in active_slugs]`.
   - If no active regions, skip and log.
   - For each source_hint in schedulers that still has budget (`remaining_budget is None or remaining_budget > 0`):
     - Call `self._generator.generate_district_queries(active_regions, source_hint=hint)`.
   - Execute district queries via `_execute_queries_parallel`.
   - Collect all articles.

   **Final:**
   - Log: `"Collection complete: {N} total articles from {M} queries"`.
   - Return flat list of all ArticleRef objects (state + district combined).

2. `async def _execute_queries_parallel(self, queries_by_source: dict[str, list[Query]]) -> list[QueryResult]`:
   Execute queries for each source in parallel using `asyncio.TaskGroup`.

   ```python
   results: list[QueryResult] = []

   async def _run_source(source_key: str, source_queries: list[Query]):
       scheduler = self._schedulers.get(source_key)
       if scheduler is None:
           return
       for query in source_queries:
           result = await scheduler.execute(query)
           results.append(result)
   ```

   Use `async with asyncio.TaskGroup() as tg:` to create one task per source_key. Each task processes its own queries sequentially through the scheduler (the scheduler handles internal rate limiting and concurrency).

   Wrap the entire TaskGroup in `try/except* Exception as eg:` to handle any ExceptionGroup -- log all exceptions but do not crash. Return whatever results were collected.

   Return `results`.

3. `async def _execute_query_list(self, scheduler: SourceScheduler, queries: list[Query]) -> list[QueryResult]`:
   Helper that runs a list of queries through a single scheduler sequentially. Returns list of QueryResults.

   For each query: `result = await scheduler.execute(query)`, append to results. If scheduler budget is exhausted mid-list (check after each query), log and break early.

Use logging module. Log at INFO level for phase transitions, query counts, active state counts, and totals. Log at DEBUG level for individual query results.

Import asyncio, logging. Import from `._models`, `._generator`, `._scheduler`, `src.data.geo_loader`.
  </action>
  <verify>
Run: `python -c "
import asyncio, logging
logging.basicConfig(level=logging.INFO)

from src.query import QueryGenerator, QueryExecutor
from src.query._scheduler import SourceScheduler, PerSecondLimiter

# Create a mock source that returns 1 article for specific queries
from src.models.article import ArticleRef
from datetime import datetime
from zoneinfo import ZoneInfo

class MockSource:
    def __init__(self, return_count=0):
        self._return_count = return_count
        self.call_count = 0
    async def search(self, query, language, country='IN', *, state='', search_term=''):
        self.call_count += 1
        if self._return_count == 0:
            return []
        return [ArticleRef(
            title=f'Heat news {self.call_count}',
            url=f'https://example.com/{self.call_count}',
            source='Test',
            date=datetime.now(ZoneInfo('Asia/Kolkata')),
            language=language or 'en',
            state=state or 'Test',
            search_term=search_term or query,
        ) for _ in range(self._return_count)]

# Quick test with mock that returns nothing (no active states -> no district queries)
mock = MockSource(return_count=0)
scheduler = SourceScheduler(
    source=mock, name='test_google', daily_limit=None,
    per_second_limiter=PerSecondLimiter(max_per_second=100.0),
    concurrency=5,
)
gen = QueryGenerator()
executor = QueryExecutor(
    schedulers={'google': scheduler},
    generator=gen,
)

# Run with just one state to keep it fast
from src.data import get_region_by_slug
raj = get_region_by_slug('rajasthan')
articles = asyncio.run(executor.run_collection([raj]))
print(f'Articles collected (empty source): {len(articles)}')
print(f'Queries made: {mock.call_count}')
print('Executor test passed')
"`
  </verify>
  <done>
QueryExecutor runs hierarchical collection: state-level queries execute first, active states are identified (none in this test since mock returns empty), district queries are skipped when no active states. Executor logs progress. No exceptions raised. Returns flat list of ArticleRef objects.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update query package exports and verify full integration</name>
  <files>src/query/__init__.py</files>
  <action>
Update `src/query/__init__.py` to:
1. Import QueryExecutor from `._executor`.
2. Add QueryExecutor to __all__.
3. Ensure all public symbols from all three submodules are exported:
   - _models.py: Query, QueryResult, build_category_query, build_broad_query, batch_districts
   - _generator.py: QueryGenerator
   - _scheduler.py: SourceScheduler, PerSecondLimiter, WindowLimiter, create_google_scheduler, create_newsdata_scheduler, create_gnews_scheduler
   - _executor.py: QueryExecutor

Update module docstring to describe the query engine package.
  </action>
  <verify>
Run: `python -c "
from src.query import (
    Query, QueryResult, build_category_query, build_broad_query, batch_districts,
    QueryGenerator,
    SourceScheduler, PerSecondLimiter, WindowLimiter,
    create_google_scheduler, create_newsdata_scheduler, create_gnews_scheduler,
    QueryExecutor,
)
print('All 12 symbols exported successfully from src.query')
print('Phase 6 query engine complete')
"`
  </verify>
  <done>
All 12 public symbols importable from src.query. The query engine package is fully integrated: QueryGenerator produces queries, SourceScheduler wraps sources with rate limiting, QueryExecutor orchestrates hierarchical execution.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.query import QueryExecutor"` -- import succeeds
2. QueryExecutor.run_collection() executes state-level queries first, then district-level for active states only
3. All three sources execute in parallel via asyncio.TaskGroup (not sequentially)
4. No exceptions propagate from run_collection() -- errors are caught and logged
5. Executor correctly identifies active states from state-level results
6. District queries are skipped when no states have active results
7. All 12 public symbols exported from src.query package
8. Logging shows phase transitions, query counts, active states, and total articles
</verification>

<success_criteria>
- QueryExecutor orchestrates hierarchical state-then-district execution
- State-level queries cover all 36 states/UTs across relevant languages
- District-level queries only execute for states with active heat news results
- All three sources execute concurrently via asyncio.TaskGroup
- Budget-exhausted sources are skipped for district queries
- Executor never raises -- all errors caught and logged
- Collection summary logged with total query and article counts
- src.query package exports all 12 public symbols
</success_criteria>

<output>
After completion, create `.planning/phases/06-query-engine-and-scheduling/06-03-SUMMARY.md`
</output>
